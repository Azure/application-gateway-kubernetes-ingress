{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction The Application Gateway Ingress Controller allows Azure Application Gateway to be used as the ingress for an Azure Kubernetes Service aka AKS cluster. As shown in the figure below, the ingress controller runs as a pod within the AKS cluster. It consumes Kubernetes Ingress Resources and converts them to an Azure Application Gateway configuration which allows the gateway to load-balance traffic to Kubernetes pods. Reporting Issues The best way to report an issue is to create a Github Issue for the project. Please include the following information when creating the issue: Subscription ID for AKS cluster. Subscription ID for Application Gateway. AKS cluster name/ARM Resource ID. Application Gateway name/ARM Resource ID. Ingress resource definition that might causing the problem. The Helm configuration used to install the ingress controller.","title":"Introduction"},{"location":"#introduction","text":"The Application Gateway Ingress Controller allows Azure Application Gateway to be used as the ingress for an Azure Kubernetes Service aka AKS cluster. As shown in the figure below, the ingress controller runs as a pod within the AKS cluster. It consumes Kubernetes Ingress Resources and converts them to an Azure Application Gateway configuration which allows the gateway to load-balance traffic to Kubernetes pods.","title":"Introduction"},{"location":"#reporting-issues","text":"The best way to report an issue is to create a Github Issue for the project. Please include the following information when creating the issue: Subscription ID for AKS cluster. Subscription ID for Application Gateway. AKS cluster name/ARM Resource ID. Application Gateway name/ARM Resource ID. Ingress resource definition that might causing the problem. The Helm configuration used to install the ingress controller.","title":"Reporting Issues"},{"location":"annotations/","text":"Annotations Introductions The Kubernetes Ingress resource can be annotated with arbitrary key/value pairs. AGIC relies on annotations to program Application Gateway features, which are not configurable via the Ingress YAML. Ingress annotations are applied to all HTTP setting, backend pools and listeners derived from an ingress resource. List of supported annotations For an Ingress resource to be observed by AGIC it must be annotated with kubernetes.io/ingress.class: azure/application-gateway . Only then AGIC will work with the Ingress resource in question. Annotation Key Value Type Default Value Allowed Values appgw.ingress.kubernetes.io/backend-path-prefix string nil appgw.ingress.kubernetes.io/ssl-redirect bool false appgw.ingress.kubernetes.io/connection-draining bool false appgw.ingress.kubernetes.io/connection-draining-timeout int32 (seconds) 30 appgw.ingress.kubernetes.io/cookie-based-affinity bool false appgw.ingress.kubernetes.io/request-timeout int32 (seconds) 30 appgw.ingress.kubernetes.io/use-private-ip bool false appgw.ingress.kubernetes.io/backend-protocol string http http , https appgw.ingress.kubernetes.io/waf-policy-for-path string Backend Path Prefix This annotation allows the backend path specified in an ingress resource to be re-written with prefix specified in this annotation. This allows users to expose services whose endpoints are different than endpoint names used to expose a service in an ingress resource. Usage appgw.ingress.kubernetes.io/backend-path-prefix : <path prefix> Example apiVersion : extensions/v1beta1 kind : Ingress metadata : name : go-server-ingress-bkprefix namespace : test-ag annotations : kubernetes.io/ingress.class : azure/application-gateway appgw.ingress.kubernetes.io/backend-path-prefix : \"/test/\" spec : rules : - http : paths : - path : /hello/ backend : serviceName : go-server-service servicePort : 80 In the example above we have defined an ingress resource named go-server-ingress-bkprefix with an annotation appgw.ingress.kubernetes.io/backend-path-prefix: \"/test/\" . The annotation tells application gateway to create an HTTP setting which will have a path prefix override for the path /hello to /test/ . NOTE: In the above example we have only one rule defined. However, the annotations is applicable to the entire ingress resource so if a user had defined multiple rules the backend path prefix would be setup for each of the paths specified. Thus, if a user wants different rules with different path prefixes (even for the same service) they would need to define different ingress resources. SSL Redirect Application Gateway can be configured to automatically redirect HTTP URLs to their HTTPS counterparts. When this annotation is present and TLS is properly configured, Kubernetes Ingress controller will create a routing rule with a redirection configuration and apply the changes to your App Gateway. The redirect created will be HTTP 301 Moved Permanently . Usage appgw.ingress.kubernetes.io/ssl-redirect : \"true\" Example apiVersion : extensions/v1beta1 kind : Ingress metadata : name : go-server-ingress-redirect namespace : test-ag annotations : kubernetes.io/ingress.class : azure/application-gateway appgw.ingress.kubernetes.io/ssl-redirect : \"true\" spec : tls : - hosts : - www.contoso.com secretName : testsecret-tls rules : - host : www.contoso.com http : paths : - backend : serviceName : websocket-repeater servicePort : 80 Connection Draining connection-draining : This annotation allows to specify whether to enable connection draining. connection-draining-timeout : This annotation allows to specify a timeout after which Application Gateway will terminate the requests to the draining backend endpoint. Usage appgw.ingress.kubernetes.io/connection-draining : \"true\" appgw.ingress.kubernetes.io/connection-draining-timeout : \"60\" Example apiVersion : extensions/v1beta1 kind : Ingress metadata : name : go-server-ingress-drain namespace : test-ag annotations : kubernetes.io/ingress.class : azure/application-gateway appgw.ingress.kubernetes.io/connection-draining : \"true\" appgw.ingress.kubernetes.io/connection-draining-timeout : \"60\" spec : rules : - http : paths : - path : /hello/ backend : serviceName : go-server-service servicePort : 80 Cookie Based Affinity This annotation allows to specify whether to enable cookie based affinity. Usage appgw.ingress.kubernetes.io/cookie-based-affinity : \"true\" Example apiVersion : extensions/v1beta1 kind : Ingress metadata : name : go-server-ingress-affinity namespace : test-ag annotations : kubernetes.io/ingress.class : azure/application-gateway appgw.ingress.kubernetes.io/cookie-based-affinity : \"true\" spec : rules : - http : paths : - path : /hello/ backend : serviceName : go-server-service servicePort : 80 Request Timeout This annotation allows to specify the request timeout in seconds after which Application Gateway will fail the request if response is not received. Usage appgw.ingress.kubernetes.io/request-timeout : \"20\" Example apiVersion : extensions/v1beta1 kind : Ingress metadata : name : go-server-ingress-timeout namespace : test-ag annotations : kubernetes.io/ingress.class : azure/application-gateway appgw.ingress.kubernetes.io/request-timeout : \"20\" spec : rules : - http : paths : - path : /hello/ backend : serviceName : go-server-service servicePort : 80 Use Private IP This annotation allows us to specify whether to expose this endpoint on Private IP of Application Gateway. Note 1) App Gateway doesn't support multiple IPs on the same port (example: 80/443). Ingress with annotation appgw.ingress.kubernetes.io/use-private-ip: \"false\" and another with appgw.ingress.kubernetes.io/use-private-ip: \"true\" on HTTP will cause AGIC to fail in updating the App Gateway. 2) For App Gateway that doesn't have a private IP, Ingresses with appgw.ingress.kubernetes.io/use-private-ip: \"true\" will be ignored. This will reflected in the controller logs and ingress events for those ingresses with NoPrivateIP warning. Usage appgw.ingress.kubernetes.io/use-private-ip : \"true\" Example apiVersion : extensions/v1beta1 kind : Ingress metadata : name : go-server-ingress-timeout namespace : test-ag annotations : kubernetes.io/ingress.class : azure/application-gateway appgw.ingress.kubernetes.io/use-private-ip : \"true\" spec : rules : - http : paths : - path : /hello/ backend : serviceName : go-server-service servicePort : 80 Backend Protocol This annotation allows us to specify the protocol that Application Gateway should use while talking to the Pods. Supported Protocols: http , https Note 1) While self-signed certificates are supported on Application Gateway, currently, AGIC only support https when Pods are using certificate signed by a well-known CA. 2) Make sure to not use port 80 with HTTPS and port 443 with HTTP on the Pods. Usage appgw.ingress.kubernetes.io/backend-protocol : \"https\" Example apiVersion : extensions/v1beta1 kind : Ingress metadata : name : go-server-ingress-timeout namespace : test-ag annotations : kubernetes.io/ingress.class : azure/application-gateway appgw.ingress.kubernetes.io/backend-protocol : \"https\" spec : rules : - http : paths : - path : /hello/ backend : serviceName : go-server-service servicePort : 443 Attach firewall policy to a host and path This annotation allows you to attach an already created WAF policy to the list paths for a host within a Kubernetes Ingress resource being annotated. The WAF policy must be created in advance. Example of using Azure Portal to create a policy: Once the policy is created, copy the URI of the policy from the address bar of Azure Portal: The URI would have the following format: /subscriptions/<YOUR-SUBSCRIPTION>/resourceGroups/<YOUR-RESOURCE-GROUP>/providers/Microsoft.Network/applicationGatewayWebApplicationFirewallPolicies/<YOUR-POLICY-NAME> Usage appgw.ingress.kubernetes.io/waf-policy-for-path : \"/subscriptions/abcd/resourceGroups/rg/providers/Microsoft.Network/applicationGatewayWebApplicationFirewallPolicies/adserver\" Example The example below will apply the WAF policy apiVersion : extensions/v1beta1 kind : Ingress metadata : name : ad-server-ingress namespace : commerce annotations : kubernetes.io/ingress.class : azure/application-gateway appgw.ingress.kubernetes.io/waf-policy-for-path : \"/subscriptions/abcd/resourceGroups/rg/providers/Microsoft.Network/applicationGatewayWebApplicationFirewallPolicies/adserver\" spec : rules : - http : paths : - path : /ad-server backend : serviceName : ad-server servicePort : 80 - path : /auth backend : serviceName : auth-server servicePort : 80 Note that the WAF policy will be applied to both /ad-server and /auth URLs.","title":"Annotations"},{"location":"annotations/#annotations","text":"","title":"Annotations"},{"location":"annotations/#introductions","text":"The Kubernetes Ingress resource can be annotated with arbitrary key/value pairs. AGIC relies on annotations to program Application Gateway features, which are not configurable via the Ingress YAML. Ingress annotations are applied to all HTTP setting, backend pools and listeners derived from an ingress resource.","title":"Introductions"},{"location":"annotations/#list-of-supported-annotations","text":"For an Ingress resource to be observed by AGIC it must be annotated with kubernetes.io/ingress.class: azure/application-gateway . Only then AGIC will work with the Ingress resource in question. Annotation Key Value Type Default Value Allowed Values appgw.ingress.kubernetes.io/backend-path-prefix string nil appgw.ingress.kubernetes.io/ssl-redirect bool false appgw.ingress.kubernetes.io/connection-draining bool false appgw.ingress.kubernetes.io/connection-draining-timeout int32 (seconds) 30 appgw.ingress.kubernetes.io/cookie-based-affinity bool false appgw.ingress.kubernetes.io/request-timeout int32 (seconds) 30 appgw.ingress.kubernetes.io/use-private-ip bool false appgw.ingress.kubernetes.io/backend-protocol string http http , https appgw.ingress.kubernetes.io/waf-policy-for-path string","title":"List of supported annotations"},{"location":"annotations/#backend-path-prefix","text":"This annotation allows the backend path specified in an ingress resource to be re-written with prefix specified in this annotation. This allows users to expose services whose endpoints are different than endpoint names used to expose a service in an ingress resource.","title":"Backend Path Prefix"},{"location":"annotations/#usage","text":"appgw.ingress.kubernetes.io/backend-path-prefix : <path prefix>","title":"Usage"},{"location":"annotations/#example","text":"apiVersion : extensions/v1beta1 kind : Ingress metadata : name : go-server-ingress-bkprefix namespace : test-ag annotations : kubernetes.io/ingress.class : azure/application-gateway appgw.ingress.kubernetes.io/backend-path-prefix : \"/test/\" spec : rules : - http : paths : - path : /hello/ backend : serviceName : go-server-service servicePort : 80 In the example above we have defined an ingress resource named go-server-ingress-bkprefix with an annotation appgw.ingress.kubernetes.io/backend-path-prefix: \"/test/\" . The annotation tells application gateway to create an HTTP setting which will have a path prefix override for the path /hello to /test/ . NOTE: In the above example we have only one rule defined. However, the annotations is applicable to the entire ingress resource so if a user had defined multiple rules the backend path prefix would be setup for each of the paths specified. Thus, if a user wants different rules with different path prefixes (even for the same service) they would need to define different ingress resources.","title":"Example"},{"location":"annotations/#ssl-redirect","text":"Application Gateway can be configured to automatically redirect HTTP URLs to their HTTPS counterparts. When this annotation is present and TLS is properly configured, Kubernetes Ingress controller will create a routing rule with a redirection configuration and apply the changes to your App Gateway. The redirect created will be HTTP 301 Moved Permanently .","title":"SSL Redirect"},{"location":"annotations/#usage_1","text":"appgw.ingress.kubernetes.io/ssl-redirect : \"true\"","title":"Usage"},{"location":"annotations/#example_1","text":"apiVersion : extensions/v1beta1 kind : Ingress metadata : name : go-server-ingress-redirect namespace : test-ag annotations : kubernetes.io/ingress.class : azure/application-gateway appgw.ingress.kubernetes.io/ssl-redirect : \"true\" spec : tls : - hosts : - www.contoso.com secretName : testsecret-tls rules : - host : www.contoso.com http : paths : - backend : serviceName : websocket-repeater servicePort : 80","title":"Example"},{"location":"annotations/#connection-draining","text":"connection-draining : This annotation allows to specify whether to enable connection draining. connection-draining-timeout : This annotation allows to specify a timeout after which Application Gateway will terminate the requests to the draining backend endpoint.","title":"Connection Draining"},{"location":"annotations/#usage_2","text":"appgw.ingress.kubernetes.io/connection-draining : \"true\" appgw.ingress.kubernetes.io/connection-draining-timeout : \"60\"","title":"Usage"},{"location":"annotations/#example_2","text":"apiVersion : extensions/v1beta1 kind : Ingress metadata : name : go-server-ingress-drain namespace : test-ag annotations : kubernetes.io/ingress.class : azure/application-gateway appgw.ingress.kubernetes.io/connection-draining : \"true\" appgw.ingress.kubernetes.io/connection-draining-timeout : \"60\" spec : rules : - http : paths : - path : /hello/ backend : serviceName : go-server-service servicePort : 80","title":"Example"},{"location":"annotations/#cookie-based-affinity","text":"This annotation allows to specify whether to enable cookie based affinity.","title":"Cookie Based Affinity"},{"location":"annotations/#usage_3","text":"appgw.ingress.kubernetes.io/cookie-based-affinity : \"true\"","title":"Usage"},{"location":"annotations/#example_3","text":"apiVersion : extensions/v1beta1 kind : Ingress metadata : name : go-server-ingress-affinity namespace : test-ag annotations : kubernetes.io/ingress.class : azure/application-gateway appgw.ingress.kubernetes.io/cookie-based-affinity : \"true\" spec : rules : - http : paths : - path : /hello/ backend : serviceName : go-server-service servicePort : 80","title":"Example"},{"location":"annotations/#request-timeout","text":"This annotation allows to specify the request timeout in seconds after which Application Gateway will fail the request if response is not received.","title":"Request Timeout"},{"location":"annotations/#usage_4","text":"appgw.ingress.kubernetes.io/request-timeout : \"20\"","title":"Usage"},{"location":"annotations/#example_4","text":"apiVersion : extensions/v1beta1 kind : Ingress metadata : name : go-server-ingress-timeout namespace : test-ag annotations : kubernetes.io/ingress.class : azure/application-gateway appgw.ingress.kubernetes.io/request-timeout : \"20\" spec : rules : - http : paths : - path : /hello/ backend : serviceName : go-server-service servicePort : 80","title":"Example"},{"location":"annotations/#use-private-ip","text":"This annotation allows us to specify whether to expose this endpoint on Private IP of Application Gateway. Note 1) App Gateway doesn't support multiple IPs on the same port (example: 80/443). Ingress with annotation appgw.ingress.kubernetes.io/use-private-ip: \"false\" and another with appgw.ingress.kubernetes.io/use-private-ip: \"true\" on HTTP will cause AGIC to fail in updating the App Gateway. 2) For App Gateway that doesn't have a private IP, Ingresses with appgw.ingress.kubernetes.io/use-private-ip: \"true\" will be ignored. This will reflected in the controller logs and ingress events for those ingresses with NoPrivateIP warning.","title":"Use Private IP"},{"location":"annotations/#usage_5","text":"appgw.ingress.kubernetes.io/use-private-ip : \"true\"","title":"Usage"},{"location":"annotations/#example_5","text":"apiVersion : extensions/v1beta1 kind : Ingress metadata : name : go-server-ingress-timeout namespace : test-ag annotations : kubernetes.io/ingress.class : azure/application-gateway appgw.ingress.kubernetes.io/use-private-ip : \"true\" spec : rules : - http : paths : - path : /hello/ backend : serviceName : go-server-service servicePort : 80","title":"Example"},{"location":"annotations/#backend-protocol","text":"This annotation allows us to specify the protocol that Application Gateway should use while talking to the Pods. Supported Protocols: http , https Note 1) While self-signed certificates are supported on Application Gateway, currently, AGIC only support https when Pods are using certificate signed by a well-known CA. 2) Make sure to not use port 80 with HTTPS and port 443 with HTTP on the Pods.","title":"Backend Protocol"},{"location":"annotations/#usage_6","text":"appgw.ingress.kubernetes.io/backend-protocol : \"https\"","title":"Usage"},{"location":"annotations/#example_6","text":"apiVersion : extensions/v1beta1 kind : Ingress metadata : name : go-server-ingress-timeout namespace : test-ag annotations : kubernetes.io/ingress.class : azure/application-gateway appgw.ingress.kubernetes.io/backend-protocol : \"https\" spec : rules : - http : paths : - path : /hello/ backend : serviceName : go-server-service servicePort : 443","title":"Example"},{"location":"annotations/#attach-firewall-policy-to-a-host-and-path","text":"This annotation allows you to attach an already created WAF policy to the list paths for a host within a Kubernetes Ingress resource being annotated. The WAF policy must be created in advance. Example of using Azure Portal to create a policy: Once the policy is created, copy the URI of the policy from the address bar of Azure Portal: The URI would have the following format: /subscriptions/<YOUR-SUBSCRIPTION>/resourceGroups/<YOUR-RESOURCE-GROUP>/providers/Microsoft.Network/applicationGatewayWebApplicationFirewallPolicies/<YOUR-POLICY-NAME>","title":"Attach firewall policy to a host and path"},{"location":"annotations/#usage_7","text":"appgw.ingress.kubernetes.io/waf-policy-for-path : \"/subscriptions/abcd/resourceGroups/rg/providers/Microsoft.Network/applicationGatewayWebApplicationFirewallPolicies/adserver\"","title":"Usage"},{"location":"annotations/#example_7","text":"The example below will apply the WAF policy apiVersion : extensions/v1beta1 kind : Ingress metadata : name : ad-server-ingress namespace : commerce annotations : kubernetes.io/ingress.class : azure/application-gateway appgw.ingress.kubernetes.io/waf-policy-for-path : \"/subscriptions/abcd/resourceGroups/rg/providers/Microsoft.Network/applicationGatewayWebApplicationFirewallPolicies/adserver\" spec : rules : - http : paths : - path : /ad-server backend : serviceName : ad-server servicePort : 80 - path : /auth backend : serviceName : auth-server servicePort : 80 Note that the WAF policy will be applied to both /ad-server and /auth URLs.","title":"Example"},{"location":"faq/","text":"Frequrently Asked Questions: [WIP] What is an Ingress Controller Can single ingress controller instance manage multiple Application Gateway What is an Ingress Controller Kubernetes allows creation of deployment and service resource to expose a group of pods internally in the cluster. To expose the same service externally, an Ingress resource is defined which provides load balancing, SSL termination and name-based virtual hosting. To satify this Ingress resource, an Ingress Controller is required which listens for any changes to Ingress resources and configures the load balancer policies. The Application Gateway Ingress Controller allows Azure Application Gateway to be used as the ingress for an Azure Kubernetes Service aka AKS cluster. Can single ingress controller instance manage multiple Application Gateway Currently, One instance of Ingress Controller can only be associated to one Application Gateway.","title":"Frequrently Asked Questions: [WIP]"},{"location":"faq/#frequrently-asked-questions-wip","text":"What is an Ingress Controller Can single ingress controller instance manage multiple Application Gateway","title":"Frequrently Asked Questions: [WIP]"},{"location":"faq/#what-is-an-ingress-controller","text":"Kubernetes allows creation of deployment and service resource to expose a group of pods internally in the cluster. To expose the same service externally, an Ingress resource is defined which provides load balancing, SSL termination and name-based virtual hosting. To satify this Ingress resource, an Ingress Controller is required which listens for any changes to Ingress resources and configures the load balancer policies. The Application Gateway Ingress Controller allows Azure Application Gateway to be used as the ingress for an Azure Kubernetes Service aka AKS cluster.","title":"What is an Ingress Controller"},{"location":"faq/#can-single-ingress-controller-instance-manage-multiple-application-gateway","text":"Currently, One instance of Ingress Controller can only be associated to one Application Gateway.","title":"Can single ingress controller instance manage multiple Application Gateway"},{"location":"troubleshooting/","text":"Troubleshooting Azure Cloud Shell is the most convenient way to troubleshoot any problems with your AKS and AGIC installation. Launch your shell from shell.azure.com or by clicking the link: Test with a simple Kubernetes app The steps below assume: - You have an AKS cluster, with Advanced Networking enabled - AGIC has been installed on the AKS cluster - You already hav an App Gateway on a VNET shared with your AKS cluster To verify that the App Gateway + AKS + AGIC installation is setup correctly, deploy the simplest possible app: cat <<EOF | kubectl apply -f - apiVersion: v1 kind: Pod metadata: name: test-agic-app-pod labels: app: test-agic-app spec: containers: - image: \"mcr.microsoft.com/dotnet/core/samples:aspnetapp\" name: aspnetapp-image ports: - containerPort: 80 protocol: TCP --- apiVersion: v1 kind: Service metadata: name: test-agic-app-service spec: selector: app: test-agic-app ports: - protocol: TCP port: 80 targetPort: 80 --- apiVersion: extensions/v1beta1 kind: Ingress metadata: name: test-agic-app-ingress annotations: kubernetes.io/ingress.class: azure/application-gateway spec: rules: - host: test.agic.contoso.com http: paths: - path: / backend: serviceName: test-agic-app-service servicePort: 80 EOF Copy and paste all lines at once from the script above into a Azure Cloud Shell . Please ensure the entire command is copied - starting with cat and including the last EOF . After a successful deployment of the app above your AKS cluster will have a new Pod, Service and an Ingress. Get the list of pods with Cloud Shell : kubectl get pods -o wide . We expect for a pod named 'test-agic-app-pod' to have been created. It will have an IP address. This address must be within the VNET of the App Gateway, which is used with AKS. Get the list of services: kubectl get services -o wide . We expect to see a service named 'test-agic-app-service'. Get the list of the ingresses: kubectl get ingress . We expect an Ingress resource named 'test-agic-app-ingress' to have been created. The resource will have a host name 'test.agic.contoso.com'. One of the pods will be AGIC. kubectl get pods will show a list of pods, one of which will begin with 'ingress-azure'. Get all logs of that pod with kubectl logs <name-of-ingress-controller-pod> to verify that we have had a successful deployment. A successful deployment would have added the following lines to the log: I0927 22:34:51.281437 1 process.go:156] Applied App Gateway config in 20.461335266s I0927 22:34:51.281585 1 process.go:165] cache: Updated with latest applied config. I0927 22:34:51.282342 1 process.go:171] END AppGateway deployment Alternatively, from Cloud Shell we can retrieve only the lines indicating successful App Gateway configuration with kubectl logs <ingress-azure-....> | grep 'Applied App Gateway config in' , where <ingress-azure....> should be the exact name of the AGIC pod. App Gateway will have the following configuration applied: Listener: Routing Rule: Backend Pool: There will be one IP address in the backend address pool and it will match the IP address of the Pod we observed earlier with kubectl get pods -o wide Finally we can use the cURL command from within Cloud Shell to establish an HTTP connection to the newly deployed app: Use kubectl get ingress to get the Public IP address of App Gateway Use curl -I -H 'test.agic.contoso.com' <publitc-ip-address-from-previous-command> A result of HTTP/1.1 200 OK indicates that the App Gateway + AKS + AGIC system is working as expected. Inspect Kubernetes Installation Pods, Services, Ingress Application Gateway Ingress Controller (AGIC) continuously monitors the folowing Kubernetes resources: Deployment or Pod , Service , Ingress The following must be in place for AGIC to function as expected: 1. AKS must have one or more healthy pods . Verify this from Cloud Shell with kubectl get pods -o wide --show-labels If you have a Pod with an apsnetapp , your output may look like this: delyan@Azure:~$ kubectl get pods -o wide --show-labels NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES LABELS aspnetapp 1 /1 Running 0 17h 10 .0.0.6 aks-agentpool-35064155-1 <none> <none> app = aspnetapp One or more services , referencing the pods above via matching selector labels. Verify this from Cloud Shell with kubectl get services -o wide delyan@Azure:~$ kubectl get services -o wide --show-labels NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE SELECTOR LABELS aspnetapp ClusterIP 10 .2.63.254 <none> 80 /TCP 17h app = aspnetapp <none> Ingress , annotated with kubernetes.io/ingress.class: azure/application-gateway , referencing the service above Verify this from Cloud Shell with kubectl get ingress -o wide --show-labels delyan@Azure:~$ kubectl get ingress -o wide --show-labels NAME HOSTS ADDRESS PORTS AGE LABELS aspnetapp * 80 17h <none> View annotations of the ingress above: kubectl get ingress aspnetapp -o yaml (substitute aspnetapp with the name of your ingress) delyan@Azure:~$ kubectl get ingress aspnetapp -o yaml apiVersion: extensions/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: azure/application-gateway name: aspnetapp spec: backend: serviceName: aspnetapp servicePort: 80 The ingress resource must be annotated with kubernetes.io/ingress.class: azure/application-gateway . Verify Observed Nampespace Get the existing namespaces in Kubernetes cluster. What namespace is your app running in? Is AGIC watching that namespace? Refer to the Multiple Namespace Support documentation on how to properly configure observed namespaces. # What namespaces exist on your cluster kubectl get namespaces # What pods are currently running kubectl get pods --all-namespaces -o wide The AGIC pod should be in the default namespace (see column NAMESPACE ). A healthy pod would have Running in the STATUS column. There should be at least one AGIC pod. # Get a list of the Application Gateway Ingress Controller pods kubectl get pods --all-namespaces --selector app = ingress-azure If the AGIC pod is not healthy ( STATUS column from the command above is not Running ): get logs to understand why: kubectl logs <pod-name> for the previous instance of the pod: kubectl logs <pod-name> --previous describe the pod to get more context: kubectl describe pod <pod-name> Do you have a Kubernetes Service and Ingress resources? # Get all services across all namespaces kubectl get service --all-namespaces -o wide # Get all ingress resources across all namespaces kubectl get ingress --all-namespaces -o wide Is your Ingress annotated with: kubernetes.io/ingress.class: azure/application-gateway ? AGIC will only watch for Kubernetes Ingress resources that have this annotation. # Get the YAML definition of a particular ingress resource kubectl get ingress --namespace <which-namespace?> <which-ingress?> -o yaml AGIC emits Kubernetes events for certain critical errors. You can view these: in your terminal via kubectl get events --sort-by=.metadata.creationTimestamp in your browser using the Kubernetes Web UI (Dashboard) Logging Levels AGIC has 3 logging levels. Level 1 is the default one and it shows minimal number of log lines. Level 5, on the other hand, would display all logs, including sanitized contents of config applied to ARM. The Kubernetes community has established 9 levels of logging for the kubectl tool. In this repository we are utilizing 3 of these, with similar semantics: Verbosity Description 1 Default log level; shows startup details, warnings and errors 3 Extended information about events and changes; lists of created objects 5 Logs marshaled objects; shows sanitized JSON config applied to ARM The verbosity levels are adjustable via the verbosityLevel variable in the helm-config.yaml file. Increase verbosity level to 5 to get the JSON config dispatched to ARM : - add verbosityLevel: 5 on a line by itself in helm-config.yaml and re-install - get logs with kubectl logs <pod-name>","title":"Troubleshooting"},{"location":"troubleshooting/#troubleshooting","text":"Azure Cloud Shell is the most convenient way to troubleshoot any problems with your AKS and AGIC installation. Launch your shell from shell.azure.com or by clicking the link:","title":"Troubleshooting"},{"location":"troubleshooting/#test-with-a-simple-kubernetes-app","text":"The steps below assume: - You have an AKS cluster, with Advanced Networking enabled - AGIC has been installed on the AKS cluster - You already hav an App Gateway on a VNET shared with your AKS cluster To verify that the App Gateway + AKS + AGIC installation is setup correctly, deploy the simplest possible app: cat <<EOF | kubectl apply -f - apiVersion: v1 kind: Pod metadata: name: test-agic-app-pod labels: app: test-agic-app spec: containers: - image: \"mcr.microsoft.com/dotnet/core/samples:aspnetapp\" name: aspnetapp-image ports: - containerPort: 80 protocol: TCP --- apiVersion: v1 kind: Service metadata: name: test-agic-app-service spec: selector: app: test-agic-app ports: - protocol: TCP port: 80 targetPort: 80 --- apiVersion: extensions/v1beta1 kind: Ingress metadata: name: test-agic-app-ingress annotations: kubernetes.io/ingress.class: azure/application-gateway spec: rules: - host: test.agic.contoso.com http: paths: - path: / backend: serviceName: test-agic-app-service servicePort: 80 EOF Copy and paste all lines at once from the script above into a Azure Cloud Shell . Please ensure the entire command is copied - starting with cat and including the last EOF . After a successful deployment of the app above your AKS cluster will have a new Pod, Service and an Ingress. Get the list of pods with Cloud Shell : kubectl get pods -o wide . We expect for a pod named 'test-agic-app-pod' to have been created. It will have an IP address. This address must be within the VNET of the App Gateway, which is used with AKS. Get the list of services: kubectl get services -o wide . We expect to see a service named 'test-agic-app-service'. Get the list of the ingresses: kubectl get ingress . We expect an Ingress resource named 'test-agic-app-ingress' to have been created. The resource will have a host name 'test.agic.contoso.com'. One of the pods will be AGIC. kubectl get pods will show a list of pods, one of which will begin with 'ingress-azure'. Get all logs of that pod with kubectl logs <name-of-ingress-controller-pod> to verify that we have had a successful deployment. A successful deployment would have added the following lines to the log: I0927 22:34:51.281437 1 process.go:156] Applied App Gateway config in 20.461335266s I0927 22:34:51.281585 1 process.go:165] cache: Updated with latest applied config. I0927 22:34:51.282342 1 process.go:171] END AppGateway deployment Alternatively, from Cloud Shell we can retrieve only the lines indicating successful App Gateway configuration with kubectl logs <ingress-azure-....> | grep 'Applied App Gateway config in' , where <ingress-azure....> should be the exact name of the AGIC pod. App Gateway will have the following configuration applied: Listener: Routing Rule: Backend Pool: There will be one IP address in the backend address pool and it will match the IP address of the Pod we observed earlier with kubectl get pods -o wide Finally we can use the cURL command from within Cloud Shell to establish an HTTP connection to the newly deployed app: Use kubectl get ingress to get the Public IP address of App Gateway Use curl -I -H 'test.agic.contoso.com' <publitc-ip-address-from-previous-command> A result of HTTP/1.1 200 OK indicates that the App Gateway + AKS + AGIC system is working as expected.","title":"Test with a simple Kubernetes app"},{"location":"troubleshooting/#inspect-kubernetes-installation","text":"","title":"Inspect Kubernetes Installation"},{"location":"troubleshooting/#pods-services-ingress","text":"Application Gateway Ingress Controller (AGIC) continuously monitors the folowing Kubernetes resources: Deployment or Pod , Service , Ingress The following must be in place for AGIC to function as expected: 1. AKS must have one or more healthy pods . Verify this from Cloud Shell with kubectl get pods -o wide --show-labels If you have a Pod with an apsnetapp , your output may look like this: delyan@Azure:~$ kubectl get pods -o wide --show-labels NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES LABELS aspnetapp 1 /1 Running 0 17h 10 .0.0.6 aks-agentpool-35064155-1 <none> <none> app = aspnetapp One or more services , referencing the pods above via matching selector labels. Verify this from Cloud Shell with kubectl get services -o wide delyan@Azure:~$ kubectl get services -o wide --show-labels NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE SELECTOR LABELS aspnetapp ClusterIP 10 .2.63.254 <none> 80 /TCP 17h app = aspnetapp <none> Ingress , annotated with kubernetes.io/ingress.class: azure/application-gateway , referencing the service above Verify this from Cloud Shell with kubectl get ingress -o wide --show-labels delyan@Azure:~$ kubectl get ingress -o wide --show-labels NAME HOSTS ADDRESS PORTS AGE LABELS aspnetapp * 80 17h <none> View annotations of the ingress above: kubectl get ingress aspnetapp -o yaml (substitute aspnetapp with the name of your ingress) delyan@Azure:~$ kubectl get ingress aspnetapp -o yaml apiVersion: extensions/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: azure/application-gateway name: aspnetapp spec: backend: serviceName: aspnetapp servicePort: 80 The ingress resource must be annotated with kubernetes.io/ingress.class: azure/application-gateway .","title":"Pods, Services, Ingress"},{"location":"troubleshooting/#verify-observed-nampespace","text":"Get the existing namespaces in Kubernetes cluster. What namespace is your app running in? Is AGIC watching that namespace? Refer to the Multiple Namespace Support documentation on how to properly configure observed namespaces. # What namespaces exist on your cluster kubectl get namespaces # What pods are currently running kubectl get pods --all-namespaces -o wide The AGIC pod should be in the default namespace (see column NAMESPACE ). A healthy pod would have Running in the STATUS column. There should be at least one AGIC pod. # Get a list of the Application Gateway Ingress Controller pods kubectl get pods --all-namespaces --selector app = ingress-azure If the AGIC pod is not healthy ( STATUS column from the command above is not Running ): get logs to understand why: kubectl logs <pod-name> for the previous instance of the pod: kubectl logs <pod-name> --previous describe the pod to get more context: kubectl describe pod <pod-name> Do you have a Kubernetes Service and Ingress resources? # Get all services across all namespaces kubectl get service --all-namespaces -o wide # Get all ingress resources across all namespaces kubectl get ingress --all-namespaces -o wide Is your Ingress annotated with: kubernetes.io/ingress.class: azure/application-gateway ? AGIC will only watch for Kubernetes Ingress resources that have this annotation. # Get the YAML definition of a particular ingress resource kubectl get ingress --namespace <which-namespace?> <which-ingress?> -o yaml AGIC emits Kubernetes events for certain critical errors. You can view these: in your terminal via kubectl get events --sort-by=.metadata.creationTimestamp in your browser using the Kubernetes Web UI (Dashboard)","title":"Verify Observed Nampespace"},{"location":"troubleshooting/#logging-levels","text":"AGIC has 3 logging levels. Level 1 is the default one and it shows minimal number of log lines. Level 5, on the other hand, would display all logs, including sanitized contents of config applied to ARM. The Kubernetes community has established 9 levels of logging for the kubectl tool. In this repository we are utilizing 3 of these, with similar semantics: Verbosity Description 1 Default log level; shows startup details, warnings and errors 3 Extended information about events and changes; lists of created objects 5 Logs marshaled objects; shows sanitized JSON config applied to ARM The verbosity levels are adjustable via the verbosityLevel variable in the helm-config.yaml file. Increase verbosity level to 5 to get the JSON config dispatched to ARM : - add verbosityLevel: 5 on a line by itself in helm-config.yaml and re-install - get logs with kubectl logs <pod-name>","title":"Logging Levels"},{"location":"tutorial/","text":"Tutorials These tutorials help illustrate the usage of Kubernetes Ingress Resources to expose an example Kubernetes service through the Azure Application Gateway over HTTP or HTTPS. Table of Contents Prerequisites Deploy guestbook application Expose services over HTTP Expose services over HTTPS Without specified hostname With specified hostname Integrate with other services Prerequisites Installed ingress-azure helm chart. Greenfield Deployment : If you are starting from scratch, refer to these installation instructions which outlines steps to deploy an AKS cluster with Application Gateway and install application gateway ingress controller on the AKS cluster. Brownfield Deployment : If you have an existing AKS cluster and Application Gateway, refer to these instructions to install application gateway ingress controller on the AKS cluster. If you want to use HTTPS on this application, you will need a x509 certificate and its private key. Deploy guestbook application The guestbook application is a canonical Kubernetes application that composes of a Web UI frontend, a backend and a Redis database. By default, guestbook exposes its application through a service with name frontend on port 80 . Without a Kubernetes Ingress Resource the service is not accessible from outside the AKS cluster. We will use the application and setup Ingress Resources to access the application through HTTP and HTTPS. Follow the instructions below to deploy the guestbook application. Download guestbook-all-in-one.yaml from here Deploy guestbook-all-in-one.yaml into your AKS cluster by running kubectl apply -f guestbook-all-in-one.yaml Now, the guestbook application has been deployed. Expose services over HTTP In order to expose the guestbook application we will using the following ingress resource: apiVersion : extensions/v1beta1 kind : Ingress metadata : name : guestbook annotations : kubernetes.io/ingress.class : azure/application-gateway spec : rules : - http : paths : - backend : serviceName : frontend servicePort : 80 This ingress will expose the frontend service of the guestbook-all-in-one deployment as a default backend of the Application Gateway. Save the above ingress resource as ing-guestbook.yaml . Deploy ing-guestbook.yaml by running: kubectl apply -f ing-guestbook.yaml Check the log of the ingress controller for deployment status. Now the guestbook application should be available. You can check this by visiting the public address of the Application Gateway. Expose services over HTTPS Without specified hostname Without specifying hostname, the guestbook service will be available on all the host-names pointing to the application gateway. Before deploying ingress, you need to create a kubernetes secret to host the certificate and private key. You can create a kubernetes secret by running kubectl create secret tls <guestbook-secret-name> --key <path-to-key> --cert <path-to-cert> Define the following ingress. In the ingress, specify the name of the secret in the secretName section. apiVersion : extensions/v1beta1 kind : Ingress metadata : name : guestbook annotations : kubernetes.io/ingress.class : azure/application-gateway spec : tls : - secretName : <guestbook-secret-name> rules : - http : paths : - backend : serviceName : frontend servicePort : 80 NOTE: Replace <guestbook-secret-name> in the above Ingress Resource with the name of your secret. Store the above Ingress Resource in a file name ing-guestbook-tls.yaml . Deploy ing-guestbook-tls.yaml by running kubectl apply -f ing-guestbook-tls.yaml Check the log of the ingress controller for deployment status. Now the guestbook application will be available on both HTTP and HTTPS. With specified hostname You can also specify the hostname on the ingress in order to multiplex TLS configurations and services. By specifying hostname, the guestbook service will only be available on the specified host. Define the following ingress. In the ingress, specify the name of the secret in the secretName section and replace the hostname in the hosts section accordingly. apiVersion : extensions/v1beta1 kind : Ingress metadata : name : guestbook annotations : kubernetes.io/ingress.class : azure/application-gateway spec : tls : - hosts : - <guestbook.contoso.com> secretName : <guestbook-secret-name> rules : - host : <guestbook.contoso.com> http : paths : - backend : serviceName : frontend servicePort : 80 Deploy ing-guestbook-tls-sni.yaml by running kubectl apply -f ing-guestbook-tls-sni.yaml Check the log of the ingress controller for deployment status. Now the guestbook application will be available on both HTTP and HTTPS only on the specified host ( <guestbook.contoso.com> in this example).","title":"Tutorials"},{"location":"tutorial/#tutorials","text":"These tutorials help illustrate the usage of Kubernetes Ingress Resources to expose an example Kubernetes service through the Azure Application Gateway over HTTP or HTTPS.","title":"Tutorials"},{"location":"tutorial/#table-of-contents","text":"Prerequisites Deploy guestbook application Expose services over HTTP Expose services over HTTPS Without specified hostname With specified hostname Integrate with other services","title":"Table of Contents"},{"location":"tutorial/#prerequisites","text":"Installed ingress-azure helm chart. Greenfield Deployment : If you are starting from scratch, refer to these installation instructions which outlines steps to deploy an AKS cluster with Application Gateway and install application gateway ingress controller on the AKS cluster. Brownfield Deployment : If you have an existing AKS cluster and Application Gateway, refer to these instructions to install application gateway ingress controller on the AKS cluster. If you want to use HTTPS on this application, you will need a x509 certificate and its private key.","title":"Prerequisites"},{"location":"tutorial/#deploy-guestbook-application","text":"The guestbook application is a canonical Kubernetes application that composes of a Web UI frontend, a backend and a Redis database. By default, guestbook exposes its application through a service with name frontend on port 80 . Without a Kubernetes Ingress Resource the service is not accessible from outside the AKS cluster. We will use the application and setup Ingress Resources to access the application through HTTP and HTTPS. Follow the instructions below to deploy the guestbook application. Download guestbook-all-in-one.yaml from here Deploy guestbook-all-in-one.yaml into your AKS cluster by running kubectl apply -f guestbook-all-in-one.yaml Now, the guestbook application has been deployed.","title":"Deploy guestbook application"},{"location":"tutorial/#expose-services-over-http","text":"In order to expose the guestbook application we will using the following ingress resource: apiVersion : extensions/v1beta1 kind : Ingress metadata : name : guestbook annotations : kubernetes.io/ingress.class : azure/application-gateway spec : rules : - http : paths : - backend : serviceName : frontend servicePort : 80 This ingress will expose the frontend service of the guestbook-all-in-one deployment as a default backend of the Application Gateway. Save the above ingress resource as ing-guestbook.yaml . Deploy ing-guestbook.yaml by running: kubectl apply -f ing-guestbook.yaml Check the log of the ingress controller for deployment status. Now the guestbook application should be available. You can check this by visiting the public address of the Application Gateway.","title":"Expose services over HTTP"},{"location":"tutorial/#expose-services-over-https","text":"","title":"Expose services over HTTPS"},{"location":"tutorial/#without-specified-hostname","text":"Without specifying hostname, the guestbook service will be available on all the host-names pointing to the application gateway. Before deploying ingress, you need to create a kubernetes secret to host the certificate and private key. You can create a kubernetes secret by running kubectl create secret tls <guestbook-secret-name> --key <path-to-key> --cert <path-to-cert> Define the following ingress. In the ingress, specify the name of the secret in the secretName section. apiVersion : extensions/v1beta1 kind : Ingress metadata : name : guestbook annotations : kubernetes.io/ingress.class : azure/application-gateway spec : tls : - secretName : <guestbook-secret-name> rules : - http : paths : - backend : serviceName : frontend servicePort : 80 NOTE: Replace <guestbook-secret-name> in the above Ingress Resource with the name of your secret. Store the above Ingress Resource in a file name ing-guestbook-tls.yaml . Deploy ing-guestbook-tls.yaml by running kubectl apply -f ing-guestbook-tls.yaml Check the log of the ingress controller for deployment status. Now the guestbook application will be available on both HTTP and HTTPS.","title":"Without specified hostname"},{"location":"tutorial/#with-specified-hostname","text":"You can also specify the hostname on the ingress in order to multiplex TLS configurations and services. By specifying hostname, the guestbook service will only be available on the specified host. Define the following ingress. In the ingress, specify the name of the secret in the secretName section and replace the hostname in the hosts section accordingly. apiVersion : extensions/v1beta1 kind : Ingress metadata : name : guestbook annotations : kubernetes.io/ingress.class : azure/application-gateway spec : tls : - hosts : - <guestbook.contoso.com> secretName : <guestbook-secret-name> rules : - host : <guestbook.contoso.com> http : paths : - backend : serviceName : frontend servicePort : 80 Deploy ing-guestbook-tls-sni.yaml by running kubectl apply -f ing-guestbook-tls-sni.yaml Check the log of the ingress controller for deployment status. Now the guestbook application will be available on both HTTP and HTTPS only on the specified host ( <guestbook.contoso.com> in this example).","title":"With specified hostname"},{"location":"developers/build/","text":"Building the controller CMake options This is a CMake-based project. Build targets include: ALL_BUILD (default target) builds appgw-ingress and dockerize target devenv builds a docker image with configured development environment vendor installs dependency using go mod in a docker container with image from devenv target appgw-ingress builds the binary for this controller in a docker container with image from devenv target dockerize builds a docker image with the binary from appgw-ingress target dockerpush pushes the docker image to a container registry with prefix defined in CMake variable <deployment_push_prefix> To run the CMake targets: mkdir build && cd build creates and enters a build directory cmake .. generates project configuration in the build directory cmake --build . to build the default target, or cmake --build . --target <target_name> to specify a target to run from above Running it locally This section outlines the environment variables and files necessary to successfully compile and run the Go binary, then connect it to an Azure Kubernetes Service . Obtain Azure Credentials In order to run the Go binary locally and control a remote AKS server, you need Azure credentials. These will be stored in a JSON file in your home directory. Follow these instructions to create the $HOME/.azure/azureAuth.json file. The file is generated via: az ad sp create-for-rbac --subscription <your-azure-subscription-id> --sdk-auth > $HOME /.azure/azureAuth.json The file will contain a JSON blob with the following shape: { \"clientId\" : \"...\" , \"clientSecret\" : \"...\" , \"subscriptionId\" : \"<your-azure-resource-group>\" , \"tenantId\" : \"...\" , \"activeDirectoryEndpointUrl\" : \"https://login.microsoftonline.com\" , \"resourceManagerEndpointUrl\" : \"https://management.azure.com/\" , \"activeDirectoryGraphResourceId\" : \"https://graph.windows.net/\" , \"sqlManagementEndpointUrl\" : \"https://management.core.windows.net:8443/\" , \"galleryEndpointUrl\" : \"https://gallery.azure.com/\" , \"managementEndpointUrl\" : \"https://management.core.windows.net/\" } Startup Script In the scripts directory you will find start.sh . This script builds and runs the ingress controller on your local machine and connects to a remote AKS cluster. A .env file in the root of the repository is required. Steps to run ingress controller: Configure: cp .env.example .env and modify the environment variables in .env to match your config Run: ./scripts/start.sh","title":"Building the controller"},{"location":"developers/build/#building-the-controller","text":"","title":"Building the controller"},{"location":"developers/build/#cmake-options","text":"This is a CMake-based project. Build targets include: ALL_BUILD (default target) builds appgw-ingress and dockerize target devenv builds a docker image with configured development environment vendor installs dependency using go mod in a docker container with image from devenv target appgw-ingress builds the binary for this controller in a docker container with image from devenv target dockerize builds a docker image with the binary from appgw-ingress target dockerpush pushes the docker image to a container registry with prefix defined in CMake variable <deployment_push_prefix> To run the CMake targets: mkdir build && cd build creates and enters a build directory cmake .. generates project configuration in the build directory cmake --build . to build the default target, or cmake --build . --target <target_name> to specify a target to run from above","title":"CMake options"},{"location":"developers/build/#running-it-locally","text":"This section outlines the environment variables and files necessary to successfully compile and run the Go binary, then connect it to an Azure Kubernetes Service .","title":"Running it locally"},{"location":"developers/build/#obtain-azure-credentials","text":"In order to run the Go binary locally and control a remote AKS server, you need Azure credentials. These will be stored in a JSON file in your home directory. Follow these instructions to create the $HOME/.azure/azureAuth.json file. The file is generated via: az ad sp create-for-rbac --subscription <your-azure-subscription-id> --sdk-auth > $HOME /.azure/azureAuth.json The file will contain a JSON blob with the following shape: { \"clientId\" : \"...\" , \"clientSecret\" : \"...\" , \"subscriptionId\" : \"<your-azure-resource-group>\" , \"tenantId\" : \"...\" , \"activeDirectoryEndpointUrl\" : \"https://login.microsoftonline.com\" , \"resourceManagerEndpointUrl\" : \"https://management.azure.com/\" , \"activeDirectoryGraphResourceId\" : \"https://graph.windows.net/\" , \"sqlManagementEndpointUrl\" : \"https://management.core.windows.net:8443/\" , \"galleryEndpointUrl\" : \"https://gallery.azure.com/\" , \"managementEndpointUrl\" : \"https://management.core.windows.net/\" }","title":"Obtain Azure Credentials"},{"location":"developers/build/#startup-script","text":"In the scripts directory you will find start.sh . This script builds and runs the ingress controller on your local machine and connects to a remote AKS cluster. A .env file in the root of the repository is required. Steps to run ingress controller: Configure: cp .env.example .env and modify the environment variables in .env to match your config Run: ./scripts/start.sh","title":"Startup Script"},{"location":"developers/contribute/","text":"Contribution Guidelines This is a Golang project. You can find the build instructions of the project in the Developer Guide . This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit https://cla.microsoft.com . When you submit a pull request, a CLA-bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA. This project has adopted the Microsoft Open Source Code of Conduct . For more information see the Code of Conduct FAQ or contact opencode@microsoft.com with any additional questions or comments.","title":"Contribution Guidelines"},{"location":"developers/contribute/#contribution-guidelines","text":"This is a Golang project. You can find the build instructions of the project in the Developer Guide . This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit https://cla.microsoft.com . When you submit a pull request, a CLA-bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA. This project has adopted the Microsoft Open Source Code of Conduct . For more information see the Code of Conduct FAQ or contact opencode@microsoft.com with any additional questions or comments.","title":"Contribution Guidelines"},{"location":"features/cookie-affinity/","text":"Enable Cookie based Affinity As outlined in the Azure Application Gateway Documentation , Application Gateway supports cookie based affinity enabling which it can direct subsequent traffic from a user session to the same server for processing. Example apiVersion : extensions/v1beta1 kind : Ingress metadata : name : guestbook annotations : kubernetes.io/ingress.class : azure/application-gateway appgw.ingress.kubernetes.io/cookie-based-affinity : \"true\" spec : rules : - http : paths : - backend : serviceName : frontend servicePort : 80","title":"Cookie affinity"},{"location":"features/cookie-affinity/#enable-cookie-based-affinity","text":"As outlined in the Azure Application Gateway Documentation , Application Gateway supports cookie based affinity enabling which it can direct subsequent traffic from a user session to the same server for processing.","title":"Enable Cookie based Affinity"},{"location":"features/cookie-affinity/#example","text":"apiVersion : extensions/v1beta1 kind : Ingress metadata : name : guestbook annotations : kubernetes.io/ingress.class : azure/application-gateway appgw.ingress.kubernetes.io/cookie-based-affinity : \"true\" spec : rules : - http : paths : - backend : serviceName : frontend servicePort : 80","title":"Example"},{"location":"features/multiple-namespaces/","text":"Multiple Namespace Support Motivation Kubernetes Namespaces make it possible for a Kubernetes cluster to be partitioned and allocated to sub-groups of a larger team. These sub-teams can then deploy and manage infrastructure with finer controls of resources, security, configuration etc. Kubernetes allows for one or more ingress resources to be defined independently within each namespace. As of version 0.7 Azure Application Gateway Kubernetes IngressController (AGIC) can ingest events from and observe multiple namespaces. Should the AKS administrator decide to use App Gateway as an ingress, all namespaces will use the same instance of App Gateway. A single installation of Ingress Controller will monitor accessible namespaces and will configure the App Gateway it is associated with. Version 0.7 of AGIC will continue to exclusively observe the default namespace, unless this is explicitly changed to one or more different namespaces in the Helm configuration (see section below). Enable multiple namespace support To enable multiple namespace support: 1. modify the helm-config.yaml file in one of the following ways: - delete the watchNamespace key entirely from helm-config.yaml - AGIC will observe all namespaces - set watchNamespace to an empty string - AGIC will observe all namespaces - add multiple namespaces separated by a comma ( watchNamespace: default,secondNamespace ) - AGIC will observe these namespaces exclusively 2. apply Helm template changes with: helm install -f helm-config.yaml application-gateway-kubernetes-ingress/ingress-azure Once deployed with the ability to observe multiple namespaces, AGIC will: - list ingress resources from all accessible namespaces - filter to ingress resources annotated with kubernetes.io/ingress.class: azure/application-gateway - compose combined App Gateway config - apply the config to the associated App Gateway via ARM Conflicting Configurations Multiple namespaced ingress resources could instruct AGIC to create conflicting configurations for a single App Gateway. (Two ingresses claiming the same domain for instance.) At the top of the hierarchy - listeners (IP address, port, and host) and routing rules (binding listener, backend pool and HTTP settings) could be created and shared by multiple namespaces/ingresses. On the other hand - paths, backend pools, HTTP settings, and TLS certificates could be created by one namespace only and duplicates will removed.. For example, consider the following duplicate ingress resources defined namespaces staging and production for www.contoso.com : apiVersion : extensions/v1beta1 kind : Ingress metadata : name : websocket-ingress namespace : staging annotations : kubernetes.io/ingress.class : azure/application-gateway spec : rules : - host : www.contoso.com http : paths : - backend : serviceName : web-service servicePort : 80 apiVersion : extensions/v1beta1 kind : Ingress metadata : name : websocket-ingress namespace : production annotations : kubernetes.io/ingress.class : azure/application-gateway spec : rules : - host : www.contoso.com http : paths : - backend : serviceName : web-service servicePort : 80 Despite the two ingress resources demanding traffic for www.contoso.com to be routed to the respective Kubernetes namespaces, only one backend can service the traffic. AGIC would create a configuration on \"first come, first served\" basis for one of the resources. If two ingresses resources are created at the same time, the one earlier in the alphabet will take precedence. From the example above we will only be able to create settings for the production ingress. App Gateway will be configured with the following resources: Listener: fl-www.contoso.com-80 Routing Rule: rr-www.contoso.com-80 Backend Pool: pool-production-contoso-web-service-80-bp-80 HTTP Settings: bp-production-contoso-web-service-80-80-websocket-ingress Health Probe: pb-production-contoso-web-service-80-websocket-ingress Note that except for listener and routing rule , the App Gateway resources created include the name of the namespace ( production ) for which they were created. If the two ingress resources are introduced into the AKS cluster at different points in time, it is likely for AGIC to end up in a scenario where it reconfigures App Gateway and re-routes traffic from namespace-B to namespace-A . For example if you added staging first, AGIC will configure App Gwy to route traffic to the staging backend pool. At a later stage, introducing production ingress, will cause AGIC to reprogram App Gwy, which will start routing traffic to the production backend pool. Restricting Access to Namespaces By default AGIC will configure App Gateway based on annotated Ingress within any namespace. Should you want to limit this behaviour you have the following options: - limit the namespaces, by explicitly defining namespaces AGIC should observe via the watchNamespace YAML key in helm-config.yaml - use Role/RoleBinding to limit AGIC to specific namespaces","title":"Multiple Namespace Support"},{"location":"features/multiple-namespaces/#multiple-namespace-support","text":"","title":"Multiple Namespace Support"},{"location":"features/multiple-namespaces/#motivation","text":"Kubernetes Namespaces make it possible for a Kubernetes cluster to be partitioned and allocated to sub-groups of a larger team. These sub-teams can then deploy and manage infrastructure with finer controls of resources, security, configuration etc. Kubernetes allows for one or more ingress resources to be defined independently within each namespace. As of version 0.7 Azure Application Gateway Kubernetes IngressController (AGIC) can ingest events from and observe multiple namespaces. Should the AKS administrator decide to use App Gateway as an ingress, all namespaces will use the same instance of App Gateway. A single installation of Ingress Controller will monitor accessible namespaces and will configure the App Gateway it is associated with. Version 0.7 of AGIC will continue to exclusively observe the default namespace, unless this is explicitly changed to one or more different namespaces in the Helm configuration (see section below).","title":"Motivation"},{"location":"features/multiple-namespaces/#enable-multiple-namespace-support","text":"To enable multiple namespace support: 1. modify the helm-config.yaml file in one of the following ways: - delete the watchNamespace key entirely from helm-config.yaml - AGIC will observe all namespaces - set watchNamespace to an empty string - AGIC will observe all namespaces - add multiple namespaces separated by a comma ( watchNamespace: default,secondNamespace ) - AGIC will observe these namespaces exclusively 2. apply Helm template changes with: helm install -f helm-config.yaml application-gateway-kubernetes-ingress/ingress-azure Once deployed with the ability to observe multiple namespaces, AGIC will: - list ingress resources from all accessible namespaces - filter to ingress resources annotated with kubernetes.io/ingress.class: azure/application-gateway - compose combined App Gateway config - apply the config to the associated App Gateway via ARM","title":"Enable multiple namespace support"},{"location":"features/multiple-namespaces/#conflicting-configurations","text":"Multiple namespaced ingress resources could instruct AGIC to create conflicting configurations for a single App Gateway. (Two ingresses claiming the same domain for instance.) At the top of the hierarchy - listeners (IP address, port, and host) and routing rules (binding listener, backend pool and HTTP settings) could be created and shared by multiple namespaces/ingresses. On the other hand - paths, backend pools, HTTP settings, and TLS certificates could be created by one namespace only and duplicates will removed.. For example, consider the following duplicate ingress resources defined namespaces staging and production for www.contoso.com : apiVersion : extensions/v1beta1 kind : Ingress metadata : name : websocket-ingress namespace : staging annotations : kubernetes.io/ingress.class : azure/application-gateway spec : rules : - host : www.contoso.com http : paths : - backend : serviceName : web-service servicePort : 80 apiVersion : extensions/v1beta1 kind : Ingress metadata : name : websocket-ingress namespace : production annotations : kubernetes.io/ingress.class : azure/application-gateway spec : rules : - host : www.contoso.com http : paths : - backend : serviceName : web-service servicePort : 80 Despite the two ingress resources demanding traffic for www.contoso.com to be routed to the respective Kubernetes namespaces, only one backend can service the traffic. AGIC would create a configuration on \"first come, first served\" basis for one of the resources. If two ingresses resources are created at the same time, the one earlier in the alphabet will take precedence. From the example above we will only be able to create settings for the production ingress. App Gateway will be configured with the following resources: Listener: fl-www.contoso.com-80 Routing Rule: rr-www.contoso.com-80 Backend Pool: pool-production-contoso-web-service-80-bp-80 HTTP Settings: bp-production-contoso-web-service-80-80-websocket-ingress Health Probe: pb-production-contoso-web-service-80-websocket-ingress Note that except for listener and routing rule , the App Gateway resources created include the name of the namespace ( production ) for which they were created. If the two ingress resources are introduced into the AKS cluster at different points in time, it is likely for AGIC to end up in a scenario where it reconfigures App Gateway and re-routes traffic from namespace-B to namespace-A . For example if you added staging first, AGIC will configure App Gwy to route traffic to the staging backend pool. At a later stage, introducing production ingress, will cause AGIC to reprogram App Gwy, which will start routing traffic to the production backend pool.","title":"Conflicting Configurations"},{"location":"features/multiple-namespaces/#restricting-access-to-namespaces","text":"By default AGIC will configure App Gateway based on annotated Ingress within any namespace. Should you want to limit this behaviour you have the following options: - limit the namespaces, by explicitly defining namespaces AGIC should observe via the watchNamespace YAML key in helm-config.yaml - use Role/RoleBinding to limit AGIC to specific namespaces","title":"Restricting Access to Namespaces"},{"location":"features/private-ip/","text":"Using Private IP for internal routing This feature allows to expose the ingress endpoint within the Virtual Network using a private IP. Pre-requisites Application Gateway with a Private IP configuration There are two ways to configure the controller to use Private IP for ingress, Assign to a particular ingress To expose a particular ingress over Private IP, use annotation appgw.ingress.kubernetes.io/use-private-ip in Ingress. Usage appgw.ingress.kubernetes.io/use-private-ip : \"true\" For App Gateways without a Private IP, Ingresses annotated with appgw.ingress.kubernetes.io/use-private-ip: \"true\" will be ignored. This will be indicated in the ingress event and AGIC pod log. Error as indicated in the Ingress Event Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning NoPrivateIP 2m ( x17 over 2m ) azure/application-gateway, prod-ingress-azure-5c9b6fcd4-bctcb Ingress default/hello-world-ingress requires Application Gateway applicationgateway3026 has a private IP address Error as indicated in AGIC Logs E0730 18 :57:37.914749 1 prune.go:65 ] Ingress default/hello-world-ingress requires Application Gateway applicationgateway3026 has a private IP address Assign Globally In case, requirement is to restrict all Ingresses to be exposed over Private IP, use appgw.usePrivateIP: true in helm config. Usage appgw : subscriptionId : <subscriptionId> resourceGroup : <resourceGroupName> name : <applicationGatewayName> usePrivateIP : true This will make the ingress controller filter the ipconfigurations for a Private IP when configuring the frontend listeners on the Application Gateway. AGIC will panic and crash if usePrivateIP: true and no Private IP is assigned. Notes: Application Gateway v2 SKU requires a Public IP. Should you require Application Gateway to be private, Attach a Network Security Group to the Application Gateway's subnet to restrict traffic.","title":"Using Private IP for internal routing"},{"location":"features/private-ip/#using-private-ip-for-internal-routing","text":"This feature allows to expose the ingress endpoint within the Virtual Network using a private IP. Pre-requisites Application Gateway with a Private IP configuration There are two ways to configure the controller to use Private IP for ingress,","title":"Using Private IP for internal routing"},{"location":"features/private-ip/#assign-to-a-particular-ingress","text":"To expose a particular ingress over Private IP, use annotation appgw.ingress.kubernetes.io/use-private-ip in Ingress.","title":"Assign to a particular ingress"},{"location":"features/private-ip/#usage","text":"appgw.ingress.kubernetes.io/use-private-ip : \"true\" For App Gateways without a Private IP, Ingresses annotated with appgw.ingress.kubernetes.io/use-private-ip: \"true\" will be ignored. This will be indicated in the ingress event and AGIC pod log. Error as indicated in the Ingress Event Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning NoPrivateIP 2m ( x17 over 2m ) azure/application-gateway, prod-ingress-azure-5c9b6fcd4-bctcb Ingress default/hello-world-ingress requires Application Gateway applicationgateway3026 has a private IP address Error as indicated in AGIC Logs E0730 18 :57:37.914749 1 prune.go:65 ] Ingress default/hello-world-ingress requires Application Gateway applicationgateway3026 has a private IP address","title":"Usage"},{"location":"features/private-ip/#assign-globally","text":"In case, requirement is to restrict all Ingresses to be exposed over Private IP, use appgw.usePrivateIP: true in helm config.","title":"Assign Globally"},{"location":"features/private-ip/#usage_1","text":"appgw : subscriptionId : <subscriptionId> resourceGroup : <resourceGroupName> name : <applicationGatewayName> usePrivateIP : true This will make the ingress controller filter the ipconfigurations for a Private IP when configuring the frontend listeners on the Application Gateway. AGIC will panic and crash if usePrivateIP: true and no Private IP is assigned. Notes: Application Gateway v2 SKU requires a Public IP. Should you require Application Gateway to be private, Attach a Network Security Group to the Application Gateway's subnet to restrict traffic.","title":"Usage"},{"location":"features/probes/","text":"Adding Health Probes to your service By default, Ingress controller will provision an HTTP GET probe for the exposed pods. The probe properties can be customized by adding a Readiness or Liveness Probe to your deployment / pod spec. With readinessProbe or livenessProbe apiVersion : extensions/v1beta1 kind : Deployment metadata : name : aspnetapp spec : replicas : 3 template : metadata : labels : service : site spec : containers : - name : aspnetapp image : mcr.microsoft.com/dotnet/core/samples:aspnetapp imagePullPolicy : IfNotPresent ports : - containerPort : 80 readinessProbe : httpGet : path : / port : 80 periodSeconds : 3 timeoutSeconds : 1 Kubernetes API Reference: * Container Probes * HttpGet Action Note: readinessProbe and livenessProbe are supported when configured with httpGet . Probing on a port other than the one exposed on the pod is currently not supported. HttpHeaders , InitialDelaySeconds , SuccessThreshold are not supported. Without readinessProbe or livenessProbe If the above probes are not provided, then Ingress Controller make an assumption that the service is reachable on Path specified for backend-path-prefix annotation or the path specified in the ingress definition for the service. Default Values for Health Probe For any property that can not be inferred by the readiness/liveness probe, Default values are set. Application Gateway Probe Property Default Value Path / Host localhost Protocol HTTP Timeout 30 Interval 30 UnhealthyThreshold 3","title":"Probes"},{"location":"features/probes/#adding-health-probes-to-your-service","text":"By default, Ingress controller will provision an HTTP GET probe for the exposed pods. The probe properties can be customized by adding a Readiness or Liveness Probe to your deployment / pod spec.","title":"Adding Health Probes to your service"},{"location":"features/probes/#with-readinessprobe-or-livenessprobe","text":"apiVersion : extensions/v1beta1 kind : Deployment metadata : name : aspnetapp spec : replicas : 3 template : metadata : labels : service : site spec : containers : - name : aspnetapp image : mcr.microsoft.com/dotnet/core/samples:aspnetapp imagePullPolicy : IfNotPresent ports : - containerPort : 80 readinessProbe : httpGet : path : / port : 80 periodSeconds : 3 timeoutSeconds : 1 Kubernetes API Reference: * Container Probes * HttpGet Action Note: readinessProbe and livenessProbe are supported when configured with httpGet . Probing on a port other than the one exposed on the pod is currently not supported. HttpHeaders , InitialDelaySeconds , SuccessThreshold are not supported.","title":"With readinessProbe or livenessProbe"},{"location":"features/probes/#without-readinessprobe-or-livenessprobe","text":"If the above probes are not provided, then Ingress Controller make an assumption that the service is reachable on Path specified for backend-path-prefix annotation or the path specified in the ingress definition for the service.","title":"Without readinessProbe or livenessProbe"},{"location":"features/probes/#default-values-for-health-probe","text":"For any property that can not be inferred by the readiness/liveness probe, Default values are set. Application Gateway Probe Property Default Value Path / Host localhost Protocol HTTP Timeout 30 Interval 30 UnhealthyThreshold 3","title":"Default Values for Health Probe"},{"location":"how-tos/continuous-deployment/","text":"Continuous Deployment with AKS and AGIC using Azure Pipelines To achieve an efficiently deployed and managed global infrastucture, it is important to setup workflows for continuous integration and deployment. Azure Devops is one of the options to achieve this goal. In following example, we setup a Azure Devops release pipeline to deploy an AKS cluster along with AGIC as ingress. This example is merely a scaffolding. You need to separately setup a build pipeline to install your application and ingress on the AKS cluster deployed as part of the release. Setup up new service connection with service pricipal Note : Skip if already have service connection with owner access for role assigment Create a service principal to use with Azure Pipelines. This service principal will have owner access to current subscription. Access will be used to perform role assigement for AGIC identity in the pipeline. az ad sp create-for-rbac -n azure-pipeline-cd --role owner # Copy the AppId and Password. We will use these in the next step. Now, create a new service connection in Azure Devops. Select \" use the full version of the service connection dialog \" option so that you can provide the newly created service principal. Create a new Azure release pipeline We have prepared an example release pipeline . This pipeline has following tasks: 1. Deploy AKS Cluster 1. Create a user assigned identity used by AGIC Pod 1. Install Helm 1. Install AAD Pod identity 1. Install AGIC 1. Install a sample application (with ingress) To use the example release pipeline, 1. Download the template and import it to your project's release pipeline. 1. Now provide the required settings for all tasks: 1. Select the correct Agent Pool 1. Select the newly created service conenction for the Create Kubernetes Cluster and Create AGIC Identity tasks. 1. Provide the values for clientId and clientSecret that will be configured as cluster credentials for the AKS cluster. You should create a separate service principal for the AKS cluster for security reasons. # create a new one and copy the appId and password to the variable section in the pipeline az ad sp create-for-rbac -n aks-cluster Click Save . Now your pipeline is all set up. Hit Create release and provide a location(Azure region) where you want the cluster to be deployed. Snapshot of how the AKS node resource group will look: If this is your first deployment, AGIC will create a new application gateway. You should be able to visit the Application Gateway's ip address to visit the sample application.","title":"Continuous Deployment with AKS and AGIC using Azure Pipelines"},{"location":"how-tos/continuous-deployment/#continuous-deployment-with-aks-and-agic-using-azure-pipelines","text":"To achieve an efficiently deployed and managed global infrastucture, it is important to setup workflows for continuous integration and deployment. Azure Devops is one of the options to achieve this goal. In following example, we setup a Azure Devops release pipeline to deploy an AKS cluster along with AGIC as ingress. This example is merely a scaffolding. You need to separately setup a build pipeline to install your application and ingress on the AKS cluster deployed as part of the release.","title":"Continuous Deployment with AKS and AGIC using Azure Pipelines"},{"location":"how-tos/continuous-deployment/#setup-up-new-service-connection-with-service-pricipal","text":"Note : Skip if already have service connection with owner access for role assigment Create a service principal to use with Azure Pipelines. This service principal will have owner access to current subscription. Access will be used to perform role assigement for AGIC identity in the pipeline. az ad sp create-for-rbac -n azure-pipeline-cd --role owner # Copy the AppId and Password. We will use these in the next step. Now, create a new service connection in Azure Devops. Select \" use the full version of the service connection dialog \" option so that you can provide the newly created service principal.","title":"Setup up new service connection with service pricipal"},{"location":"how-tos/continuous-deployment/#create-a-new-azure-release-pipeline","text":"We have prepared an example release pipeline . This pipeline has following tasks: 1. Deploy AKS Cluster 1. Create a user assigned identity used by AGIC Pod 1. Install Helm 1. Install AAD Pod identity 1. Install AGIC 1. Install a sample application (with ingress) To use the example release pipeline, 1. Download the template and import it to your project's release pipeline. 1. Now provide the required settings for all tasks: 1. Select the correct Agent Pool 1. Select the newly created service conenction for the Create Kubernetes Cluster and Create AGIC Identity tasks. 1. Provide the values for clientId and clientSecret that will be configured as cluster credentials for the AKS cluster. You should create a separate service principal for the AKS cluster for security reasons. # create a new one and copy the appId and password to the variable section in the pipeline az ad sp create-for-rbac -n aks-cluster Click Save . Now your pipeline is all set up. Hit Create release and provide a location(Azure region) where you want the cluster to be deployed. Snapshot of how the AKS node resource group will look: If this is your first deployment, AGIC will create a new application gateway. You should be able to visit the Application Gateway's ip address to visit the sample application.","title":"Create a new Azure release pipeline"},{"location":"how-tos/dns/","text":"Automate DNS updates When a hostname is specified in the Kubrenetes Ingress resource's rules, it can be used to automaticall create DNS records for the given domain and App Gateway's IP address. To achieve this the ExternalDNS Kubernetes app is required. ExternalDNS in installable via a Helm chart . The following document provides a tutorial on setting up ExternalDNS with an Azure DNS. Below is a sample Ingress resource, annotated with kubernetes.io/ingress.class: azure/application-gateway , which configures aplpha.contoso.com . apiVersion : extensions/v1beta1 kind : Ingress metadata : name : websocket-ingress namespace : alpha annotations : kubernetes.io/ingress.class : azure/application-gateway spec : rules : - host : alpha.contoso.com http : paths : - path : / backend : serviceName : contoso-service servicePort : 80 Application Gateway Ingress Controller (AGIC) automatically recognizes the public IP address assigned to the Application Gateway it is associated with, and sets this IP ( 1.2.3.4 ) on the Ingress resource as shown below: $ kubectl get ingress -A NAMESPACE NAME HOSTS ADDRESS PORTS AGE alpha alpha-ingress alpha.contoso.com 1 .2.3.4 80 8m55s beta beta-ingress beta.contoso.com 1 .2.3.4 80 8m54s Once the Ingresses contain both host and adrress, ExternalDNS will provision these to the DNS system it has been associated with and authorized for.","title":"Automate DNS updates"},{"location":"how-tos/dns/#automate-dns-updates","text":"When a hostname is specified in the Kubrenetes Ingress resource's rules, it can be used to automaticall create DNS records for the given domain and App Gateway's IP address. To achieve this the ExternalDNS Kubernetes app is required. ExternalDNS in installable via a Helm chart . The following document provides a tutorial on setting up ExternalDNS with an Azure DNS. Below is a sample Ingress resource, annotated with kubernetes.io/ingress.class: azure/application-gateway , which configures aplpha.contoso.com . apiVersion : extensions/v1beta1 kind : Ingress metadata : name : websocket-ingress namespace : alpha annotations : kubernetes.io/ingress.class : azure/application-gateway spec : rules : - host : alpha.contoso.com http : paths : - path : / backend : serviceName : contoso-service servicePort : 80 Application Gateway Ingress Controller (AGIC) automatically recognizes the public IP address assigned to the Application Gateway it is associated with, and sets this IP ( 1.2.3.4 ) on the Ingress resource as shown below: $ kubectl get ingress -A NAMESPACE NAME HOSTS ADDRESS PORTS AGE alpha alpha-ingress alpha.contoso.com 1 .2.3.4 80 8m55s beta beta-ingress beta.contoso.com 1 .2.3.4 80 8m54s Once the Ingresses contain both host and adrress, ExternalDNS will provision these to the DNS system it has been associated with and authorized for.","title":"Automate DNS updates"},{"location":"how-tos/helm-upgrade/","text":"Upgrading AGIC using Helm The Azure Application Gateway Ingress Controller for Kubernetes (AGIC) can be upgraded using a Helm repository hosted on Azure Storage. Before we begin the upgrade procedure, ensure that you have added the required repository: View your currently added Helm repositories with: helm repo list Add the AGIC repo with: helm repo add \\ application-gateway-kubernetes-ingress \\ https://appgwingress.blob.core.windows.net/ingress-azure-helm-package/ Upgrade Refresh the AGIC Helm repository to get the latest release: helm repo update View available versions of the application-gateway-kubernetes-ingress chart: helm search -l application-gateway-kubernetes-ingress Sample response: NAME CHART VERSION APP VERSION DESCRIPTION application-gateway-kubernetes-ingress/ingress-azure 1 .0.0 1 .0.0 Use Azure Application Gateway as the ingress for an Azure... application-gateway-kubernetes-ingress/ingress-azure 0 .7.0-rc1 0 .7.0-rc1 Use Azure Application Gateway as the ingress for an Azure... application-gateway-kubernetes-ingress/ingress-azure 0 .6.0 0 .6.0 Use Azure Application Gateway as the ingress for an Azure... Latest available version from the list above is: 0.7.0-rc1 View the Helm charts currently installed: helm list Sample response: NAME REVISION UPDATED STATUS CHART APP VERSION NAMESPACE odd-billygoat 22 Fri Nov 08 15 :56:06 2019 FAILED ingress-azure-1.0.0 1 .0.0 default The Helm chart installation from the sample response above is named odd-billygoat . We will use this name for the rest of the commands. Your actual deployment name will most likely differ. Upgrade the Helm deployment to a new version: helm upgrade \\ odd-billygoat \\ application-gateway-kubernetes-ingress/ingress-azure \\ --version 1 .0.0 Rollback Should the Helm deployment fail, you can rollback to a previous release. Get the last known healthy release number: helm history odd-billygoat Sample output: REVISION UPDATED STATUS CHART DESCRIPTION 1 Mon Jun 17 13 :49:42 2019 DEPLOYED ingress-azure-0.6.0 Install complete 2 Fri Jun 21 15 :56:06 2019 FAILED ingress-azure-xx xxxx From the sample output of the helm history command it looks like the last successful deployment of our odd-billygoat was revision 1 Rollback to the last successful revision: helm rollback odd-billygoat 1","title":"Upgrading AGIC using Helm"},{"location":"how-tos/helm-upgrade/#upgrading-agic-using-helm","text":"The Azure Application Gateway Ingress Controller for Kubernetes (AGIC) can be upgraded using a Helm repository hosted on Azure Storage. Before we begin the upgrade procedure, ensure that you have added the required repository: View your currently added Helm repositories with: helm repo list Add the AGIC repo with: helm repo add \\ application-gateway-kubernetes-ingress \\ https://appgwingress.blob.core.windows.net/ingress-azure-helm-package/","title":"Upgrading AGIC using Helm"},{"location":"how-tos/helm-upgrade/#upgrade","text":"Refresh the AGIC Helm repository to get the latest release: helm repo update View available versions of the application-gateway-kubernetes-ingress chart: helm search -l application-gateway-kubernetes-ingress Sample response: NAME CHART VERSION APP VERSION DESCRIPTION application-gateway-kubernetes-ingress/ingress-azure 1 .0.0 1 .0.0 Use Azure Application Gateway as the ingress for an Azure... application-gateway-kubernetes-ingress/ingress-azure 0 .7.0-rc1 0 .7.0-rc1 Use Azure Application Gateway as the ingress for an Azure... application-gateway-kubernetes-ingress/ingress-azure 0 .6.0 0 .6.0 Use Azure Application Gateway as the ingress for an Azure... Latest available version from the list above is: 0.7.0-rc1 View the Helm charts currently installed: helm list Sample response: NAME REVISION UPDATED STATUS CHART APP VERSION NAMESPACE odd-billygoat 22 Fri Nov 08 15 :56:06 2019 FAILED ingress-azure-1.0.0 1 .0.0 default The Helm chart installation from the sample response above is named odd-billygoat . We will use this name for the rest of the commands. Your actual deployment name will most likely differ. Upgrade the Helm deployment to a new version: helm upgrade \\ odd-billygoat \\ application-gateway-kubernetes-ingress/ingress-azure \\ --version 1 .0.0","title":"Upgrade"},{"location":"how-tos/helm-upgrade/#rollback","text":"Should the Helm deployment fail, you can rollback to a previous release. Get the last known healthy release number: helm history odd-billygoat Sample output: REVISION UPDATED STATUS CHART DESCRIPTION 1 Mon Jun 17 13 :49:42 2019 DEPLOYED ingress-azure-0.6.0 Install complete 2 Fri Jun 21 15 :56:06 2019 FAILED ingress-azure-xx xxxx From the sample output of the helm history command it looks like the last successful deployment of our odd-billygoat was revision 1 Rollback to the last successful revision: helm rollback odd-billygoat 1","title":"Rollback"},{"location":"how-tos/lets-encrypt/","text":"Certificate issuance with LetsEncrypt.org This section configures your AKS to leverage LetsEncrypt.org and automatically obtain a TLS/SSL certificate for your domain. The certificate will be installed on Application Gateway, which will perform SSL/TLS termination for your AKS cluster. The setup described here uses the cert-manager Kubernetes add-on, which automates the creation and management of certificates. Follow the steps below to install cert-manager on your existing AKS cluster. Helm Chart Run the following script to install the cert-manager helm chart. This will: create a new cert-manager namespace on your AKS create the following CRDs: Certificate, Challenge, ClusterIssuer, Issuer, Order install cert-manager chart (from docs.cert-manager.io) # Install the CustomResourceDefinition resources separately # Note: --validate=false is required per https://github.com/jetstack/cert-manager/issues/2208#issuecomment-541311021 kubectl apply -f https://raw.githubusercontent.com/jetstack/cert-manager/release-0.11/deploy/manifests/00-crds.yaml --validate = false # Create the namespace for cert-manager kubectl create namespace cert-manager # Label the cert-manager namespace to disable resource validation kubectl label namespace cert-manager cert-manager.io/disable-validation = true # Add the Jetstack Helm repository helm repo add jetstack https://charts.jetstack.io # Update your local Helm chart repository cache helm repo update # Install v0.11 of cert-manager Helm chart helm install \\ --name cert-manager \\ --namespace cert-manager \\ --version v0.11.0 \\ jetstack/cert-manager ClusterIssuer Resource Create a ClusterIssuer resource. It is required by cert-manager to represent the Lets Encrypt certificate authority where the signed certificates will be obtained. By using the non-namespaced ClusterIssuer resource, cert-manager will issue certificates that can be consumed from multiple namespaces. Let\u2019s Encrypt uses the ACME protocol to verify that you control a given domain name and to issue you a certificate. More details on configuring ClusterIssuer properties here . ClusterIssuer will instruct cert-manager to issue certificates using the Lets Encrypt staging environment used for testing (the root certificate not present in browser/client trust stores). The default challenge type in the YAML below is http01 . Other challenges are documented on letsencrypt.org - Challenge Types IMPORTANT: Update <YOUR.EMAIL@ADDRESS> in the YAML below kubectl apply -f - <<EOF apiVersion: cert-manager.io/v1alpha2 kind: ClusterIssuer metadata: name: letsencrypt-staging spec: acme: # You must replace this email address with your own. # Let's Encrypt will use this to contact you about expiring # certificates, and issues related to your account. email: <YOUR.EMAIL@ADDRESS> # ACME server URL for Let\u2019s Encrypt\u2019s staging environment. # The staging environment will not issue trusted certificates but is # used to ensure that the verification process is working properly # before moving to production server: https://acme-staging-v02.api.letsencrypt.org/directory privateKeySecretRef: # Secret resource used to store the account's private key. name: letsencrypt-secret # Enable the HTTP-01 challenge provider # you prove ownership of a domain by ensuring that a particular # file is present at the domain solvers: - http01: ingress: class: azure/application-gateway EOF Deploy App Create an Ingress resource to Expose the guestbook application using the Application Gateway with the Lets Encrypt Certificate. Ensure you Application Gateway has a public Frontend IP configuration with a DNS name (either using the default azure.com domain, or provision a Azure DNS Zone service, and assign your own custom domain). Note the annotation cert-manager.io/cluster-issuer: letsencrypt-staging , which tells cert-manager to process the tagged Ingress resource. IMPORTANT: Update <PLACEHOLDERS.COM> in the YAML below with your own domain (or the Application Gateway one, for example 'kh-aks-ingress.westeurope.cloudapp.azure.com') kubectl apply -f - <<EOF apiVersion: extensions/v1beta1 kind: Ingress metadata: name: guestbook-letsencrypt-staging annotations: kubernetes.io/ingress.class: azure/application-gateway cert-manager.io/cluster-issuer: letsencrypt-staging cert-manager.io/acme-challenge-type: http01 spec: tls: - hosts: - <PLACEHOLDERS.COM> secretName: guestbook-secret-name rules: - host: <PLACEHOLDERS.COM> http: paths: - backend: serviceName: frontend servicePort: 80 EOF Use kubectl describe clusterissuer letsencrypt-staging to view the state of status of the ACME account registration. Use kubectl get secret guestbook-secret-name -o yaml to view the certificate issued. After a few seconds, you can access the guestbook service through the Application Gateway HTTPS url using the automatically issued staging Lets Encrypt certificate. Your browser may warn you of an invalid cert authority. The staging certificate is issued by CN=Fake LE Intermediate X1 . This is an indication that the system worked as expected and you are ready for your production certificate. Production Certificate Once your staging certificate is setup successfully you can switch to a production ACME server: Replace the staging annotation on your Ingress resource with: cert-manager.io/cluster-issuer: letsencrypt-prod Delete the existing staging ClusterIssuer you created in the previous step and create a new one by replacing the ACME server from the ClusterIssuer YAML above with https://acme-v02.api.letsencrypt.org/directory Certificate Expiration and Renewal Before the Lets Encrypt certificate expires, cert-manager will automatically update the certificate in the Kubernetes secret store. At that point, Application Gateway Ingress Controller will apply the updated secret referenced in the ingress resources it is using to configure the Application Gateway.","title":"Certificate issuance with LetsEncrypt.org"},{"location":"how-tos/lets-encrypt/#certificate-issuance-with-letsencryptorg","text":"This section configures your AKS to leverage LetsEncrypt.org and automatically obtain a TLS/SSL certificate for your domain. The certificate will be installed on Application Gateway, which will perform SSL/TLS termination for your AKS cluster. The setup described here uses the cert-manager Kubernetes add-on, which automates the creation and management of certificates. Follow the steps below to install cert-manager on your existing AKS cluster. Helm Chart Run the following script to install the cert-manager helm chart. This will: create a new cert-manager namespace on your AKS create the following CRDs: Certificate, Challenge, ClusterIssuer, Issuer, Order install cert-manager chart (from docs.cert-manager.io) # Install the CustomResourceDefinition resources separately # Note: --validate=false is required per https://github.com/jetstack/cert-manager/issues/2208#issuecomment-541311021 kubectl apply -f https://raw.githubusercontent.com/jetstack/cert-manager/release-0.11/deploy/manifests/00-crds.yaml --validate = false # Create the namespace for cert-manager kubectl create namespace cert-manager # Label the cert-manager namespace to disable resource validation kubectl label namespace cert-manager cert-manager.io/disable-validation = true # Add the Jetstack Helm repository helm repo add jetstack https://charts.jetstack.io # Update your local Helm chart repository cache helm repo update # Install v0.11 of cert-manager Helm chart helm install \\ --name cert-manager \\ --namespace cert-manager \\ --version v0.11.0 \\ jetstack/cert-manager ClusterIssuer Resource Create a ClusterIssuer resource. It is required by cert-manager to represent the Lets Encrypt certificate authority where the signed certificates will be obtained. By using the non-namespaced ClusterIssuer resource, cert-manager will issue certificates that can be consumed from multiple namespaces. Let\u2019s Encrypt uses the ACME protocol to verify that you control a given domain name and to issue you a certificate. More details on configuring ClusterIssuer properties here . ClusterIssuer will instruct cert-manager to issue certificates using the Lets Encrypt staging environment used for testing (the root certificate not present in browser/client trust stores). The default challenge type in the YAML below is http01 . Other challenges are documented on letsencrypt.org - Challenge Types IMPORTANT: Update <YOUR.EMAIL@ADDRESS> in the YAML below kubectl apply -f - <<EOF apiVersion: cert-manager.io/v1alpha2 kind: ClusterIssuer metadata: name: letsencrypt-staging spec: acme: # You must replace this email address with your own. # Let's Encrypt will use this to contact you about expiring # certificates, and issues related to your account. email: <YOUR.EMAIL@ADDRESS> # ACME server URL for Let\u2019s Encrypt\u2019s staging environment. # The staging environment will not issue trusted certificates but is # used to ensure that the verification process is working properly # before moving to production server: https://acme-staging-v02.api.letsencrypt.org/directory privateKeySecretRef: # Secret resource used to store the account's private key. name: letsencrypt-secret # Enable the HTTP-01 challenge provider # you prove ownership of a domain by ensuring that a particular # file is present at the domain solvers: - http01: ingress: class: azure/application-gateway EOF Deploy App Create an Ingress resource to Expose the guestbook application using the Application Gateway with the Lets Encrypt Certificate. Ensure you Application Gateway has a public Frontend IP configuration with a DNS name (either using the default azure.com domain, or provision a Azure DNS Zone service, and assign your own custom domain). Note the annotation cert-manager.io/cluster-issuer: letsencrypt-staging , which tells cert-manager to process the tagged Ingress resource. IMPORTANT: Update <PLACEHOLDERS.COM> in the YAML below with your own domain (or the Application Gateway one, for example 'kh-aks-ingress.westeurope.cloudapp.azure.com') kubectl apply -f - <<EOF apiVersion: extensions/v1beta1 kind: Ingress metadata: name: guestbook-letsencrypt-staging annotations: kubernetes.io/ingress.class: azure/application-gateway cert-manager.io/cluster-issuer: letsencrypt-staging cert-manager.io/acme-challenge-type: http01 spec: tls: - hosts: - <PLACEHOLDERS.COM> secretName: guestbook-secret-name rules: - host: <PLACEHOLDERS.COM> http: paths: - backend: serviceName: frontend servicePort: 80 EOF Use kubectl describe clusterissuer letsencrypt-staging to view the state of status of the ACME account registration. Use kubectl get secret guestbook-secret-name -o yaml to view the certificate issued. After a few seconds, you can access the guestbook service through the Application Gateway HTTPS url using the automatically issued staging Lets Encrypt certificate. Your browser may warn you of an invalid cert authority. The staging certificate is issued by CN=Fake LE Intermediate X1 . This is an indication that the system worked as expected and you are ready for your production certificate. Production Certificate Once your staging certificate is setup successfully you can switch to a production ACME server: Replace the staging annotation on your Ingress resource with: cert-manager.io/cluster-issuer: letsencrypt-prod Delete the existing staging ClusterIssuer you created in the previous step and create a new one by replacing the ACME server from the ClusterIssuer YAML above with https://acme-v02.api.letsencrypt.org/directory Certificate Expiration and Renewal Before the Lets Encrypt certificate expires, cert-manager will automatically update the certificate in the Kubernetes secret store. At that point, Application Gateway Ingress Controller will apply the updated secret referenced in the ingress resources it is using to configure the Application Gateway.","title":"Certificate issuance with LetsEncrypt.org"},{"location":"how-tos/scale-applications-using-appgw-metrics/","text":"Scale your Applications using Application Gateway Metrics (Beta) As incoming traffic increases, it becomes crucial to scale up your applications based on the demand. In the following tutorial, we explain how you can use Application Gateway's AvgRequestCountPerHealthyHost metric to scale up your application. AvgRequestCountPerHealthyHost is measure of average request that are sent to a specific backend pool and backend http setting combination. We are going to use following two components: Azure K8S Metric Adapter - We will using the metric adapter to expose Application Gateway metrics through the metric server. Horizontal Pod Autoscaler - We will use HPA to use Application Gateway metrics and target a deployment for scaling. Setting up Azure K8S Metric Adapter We will first create an Azure AAD service principal and assign it Monitoring Reader access over Application Gateway's resource group. Paste the following lines in your Azure Cloud Shell : applicationGatewayGroupName = \"<application-gateway-group-id>\" applicationGatewayGroupId = $( az group show -g $applicationGatewayGroupName -o tsv --query \"id\" ) az ad sp create-for-rbac -n \"azure-k8s-metric-adapter-sp\" --role \"Monitoring Reader\" --scopes applicationGatewayGroupId Now, We will deploy the Azure K8S Metric Adapter using the AAD service principal created above. kubectl create namespace custom-metrics # use values from service principle created above to create secret kubectl create secret generic azure-k8s-metrics-adapter -n custom-metrics \\ --from-literal = azure-tenant-id = <tenantid> \\ --from-literal = azure-client-id = <clientid> \\ --from-literal = azure-client-secret = <secret> kubectl apply -f kubectl apply -f https://raw.githubusercontent.com/Azure/azure-k8s-metrics-adapter/master/deploy/adapter.yaml -n custom-metrics We will create an ExternalMetric resource with name appgw-request-count-metric . This will instruct the metric adapter to expose AvgRequestCountPerHealthyHost metric for myApplicationGateway resource in myResourceGroup resource group. You can use the filter field to target a specific backend pool and backend http setting in the Application Gateway. Copy paste this YAML content in external-metric.yaml and apply with kubectl apply -f external-metric.yaml . apiVersion : azure.com/v1alpha2 kind : ExternalMetric metadata : name : appgw-request-count-metric spec : type : azuremonitor azure : resourceGroup : myResourceGroup # replace with your application gateway's resource group name resourceName : myApplicationGateway # replace with your application gateway's name resourceProviderNamespace : Microsoft.Network resourceType : applicationGateways metric : metricName : AvgRequestCountPerHealthyHost aggregation : Average filter : BackendSettingsPool eq '<backend-pool-name>~<backend-http-setting-name>' # optional You can now make a request to the metric server to see if our new metric is getting exposed: kubectl get --raw \"/apis/external.metrics.k8s.io/v1beta1/namespaces/default/appgw-request-count-metric\" # Sample Output # { # \"kind\": \"ExternalMetricValueList\", # \"apiVersion\": \"external.metrics.k8s.io/v1beta1\", # \"metadata\": # { # \"selfLink\": \"/apis/external.metrics.k8s.io/v1beta1/namespaces/default/appgw-request-count-metric\", # }, # \"items\": # [ # { # \"metricName\": \"appgw-request-count-metric\", # \"metricLabels\": null, # \"timestamp\": \"2019-11-05T00:18:51Z\", # \"value\": \"30\", # }, # ], # } Using the new metric to scale up our deployment Once we are able to expose appgw-request-count-metric through the metric server, We are ready to use Horizontal Pod Autoscaler to scale up our target deployment. In following example, we will target a sample deployment aspnet . We will scale up Pods when appgw-request-count-metric > 200 per Pod upto a max of 10 Pods. Replace your target deployment name and apply the following auto scale configuration. Copy paste this YAML content in autoscale-config.yaml and apply with kubectl apply -f autoscale-config.yaml . apiVersion : autoscaling/v2beta1 kind : HorizontalPodAutoscaler metadata : name : deployment-scaler spec : scaleTargetRef : apiVersion : extensions/v1beta1 kind : Deployment name : aspnet # replace with your deployment's name minReplicas : 1 maxReplicas : 10 metrics : - type : External external : metricName : appgw-request-count-metric targetAverageValue : 200 Test your configuration by using a load test tools like apache bench: ab -n10000 http://<application-gateway-ip-address>/","title":"Scale your Applications using Application Gateway Metrics (Beta)"},{"location":"how-tos/scale-applications-using-appgw-metrics/#scale-your-applications-using-application-gateway-metrics-beta","text":"As incoming traffic increases, it becomes crucial to scale up your applications based on the demand. In the following tutorial, we explain how you can use Application Gateway's AvgRequestCountPerHealthyHost metric to scale up your application. AvgRequestCountPerHealthyHost is measure of average request that are sent to a specific backend pool and backend http setting combination. We are going to use following two components: Azure K8S Metric Adapter - We will using the metric adapter to expose Application Gateway metrics through the metric server. Horizontal Pod Autoscaler - We will use HPA to use Application Gateway metrics and target a deployment for scaling.","title":"Scale your Applications using Application Gateway Metrics (Beta)"},{"location":"how-tos/scale-applications-using-appgw-metrics/#setting-up-azure-k8s-metric-adapter","text":"We will first create an Azure AAD service principal and assign it Monitoring Reader access over Application Gateway's resource group. Paste the following lines in your Azure Cloud Shell : applicationGatewayGroupName = \"<application-gateway-group-id>\" applicationGatewayGroupId = $( az group show -g $applicationGatewayGroupName -o tsv --query \"id\" ) az ad sp create-for-rbac -n \"azure-k8s-metric-adapter-sp\" --role \"Monitoring Reader\" --scopes applicationGatewayGroupId Now, We will deploy the Azure K8S Metric Adapter using the AAD service principal created above. kubectl create namespace custom-metrics # use values from service principle created above to create secret kubectl create secret generic azure-k8s-metrics-adapter -n custom-metrics \\ --from-literal = azure-tenant-id = <tenantid> \\ --from-literal = azure-client-id = <clientid> \\ --from-literal = azure-client-secret = <secret> kubectl apply -f kubectl apply -f https://raw.githubusercontent.com/Azure/azure-k8s-metrics-adapter/master/deploy/adapter.yaml -n custom-metrics We will create an ExternalMetric resource with name appgw-request-count-metric . This will instruct the metric adapter to expose AvgRequestCountPerHealthyHost metric for myApplicationGateway resource in myResourceGroup resource group. You can use the filter field to target a specific backend pool and backend http setting in the Application Gateway. Copy paste this YAML content in external-metric.yaml and apply with kubectl apply -f external-metric.yaml . apiVersion : azure.com/v1alpha2 kind : ExternalMetric metadata : name : appgw-request-count-metric spec : type : azuremonitor azure : resourceGroup : myResourceGroup # replace with your application gateway's resource group name resourceName : myApplicationGateway # replace with your application gateway's name resourceProviderNamespace : Microsoft.Network resourceType : applicationGateways metric : metricName : AvgRequestCountPerHealthyHost aggregation : Average filter : BackendSettingsPool eq '<backend-pool-name>~<backend-http-setting-name>' # optional You can now make a request to the metric server to see if our new metric is getting exposed: kubectl get --raw \"/apis/external.metrics.k8s.io/v1beta1/namespaces/default/appgw-request-count-metric\" # Sample Output # { # \"kind\": \"ExternalMetricValueList\", # \"apiVersion\": \"external.metrics.k8s.io/v1beta1\", # \"metadata\": # { # \"selfLink\": \"/apis/external.metrics.k8s.io/v1beta1/namespaces/default/appgw-request-count-metric\", # }, # \"items\": # [ # { # \"metricName\": \"appgw-request-count-metric\", # \"metricLabels\": null, # \"timestamp\": \"2019-11-05T00:18:51Z\", # \"value\": \"30\", # }, # ], # }","title":"Setting up Azure K8S Metric Adapter"},{"location":"how-tos/scale-applications-using-appgw-metrics/#using-the-new-metric-to-scale-up-our-deployment","text":"Once we are able to expose appgw-request-count-metric through the metric server, We are ready to use Horizontal Pod Autoscaler to scale up our target deployment. In following example, we will target a sample deployment aspnet . We will scale up Pods when appgw-request-count-metric > 200 per Pod upto a max of 10 Pods. Replace your target deployment name and apply the following auto scale configuration. Copy paste this YAML content in autoscale-config.yaml and apply with kubectl apply -f autoscale-config.yaml . apiVersion : autoscaling/v2beta1 kind : HorizontalPodAutoscaler metadata : name : deployment-scaler spec : scaleTargetRef : apiVersion : extensions/v1beta1 kind : Deployment name : aspnet # replace with your deployment's name minReplicas : 1 maxReplicas : 10 metrics : - type : External external : metricName : appgw-request-count-metric targetAverageValue : 200 Test your configuration by using a load test tools like apache bench: ab -n10000 http://<application-gateway-ip-address>/","title":"Using the new metric to scale up our deployment"},{"location":"how-tos/websockets/","text":"Expose a WebSocket server As outlined in the Application Gateway v2 documentation - it provides native support for the WebSocket and HTTP/2 protocols . Please note, that for both Application Gateway and the Kubernetes Ingress - there is no user-configurable setting to selectively enable or disable WebSocket support. The Kubernetes deployment YAML below shows the minimum configuration used to deploy a WebSocket server, which is the same as deploying a regular web server: apiVersion : apps/v1 kind : Deployment metadata : name : websocket-server spec : selector : matchLabels : app : ws-app replicas : 2 template : metadata : labels : app : ws-app spec : containers : - name : websocket-app imagePullPolicy : Always image : your-container-repo.azurecr.io/websockets-app ports : - containerPort : 8888 imagePullSecrets : - name : azure-container-registry-credentials --- apiVersion : v1 kind : Service metadata : name : websocket-app-service spec : selector : app : ws-app ports : - protocol : TCP port : 80 targetPort : 8888 --- apiVersion : extensions/v1beta1 kind : Ingress metadata : name : websocket-repeater annotations : kubernetes.io/ingress.class : azure/application-gateway spec : rules : - host : ws.contoso.com http : paths : - backend : serviceName : websocket-app-service servicePort : 80 Given that all the prerequisites are fulfilled, and you have an App Gateway controlled by a K8s Ingress in your AKS, the deployment above would result in a WebSockets server exposed on port 80 of your App Gateway's public IP and the ws.contoso.com domain. The following cURL command would test the WebSocket server deployment: curl -i -N -H \"Connection: Upgrade\" \\ -H \"Upgrade: websocket\" \\ -H \"Origin: http://localhost\" \\ -H \"Host: ws.contoso.com\" \\ -H \"Sec-Websocket-Version: 13\" \\ -H \"Sec-WebSocket-Key: 123\" \\ http://1.2.3.4:80/ws WebSocket Health Probes If your deployment does not explicitly define health probes, App Gateway would attempt an HTTP GET on your WebSocket server endpoint. Depending on the server implementation ( here is one we love ) WebSocket specific headers may be required ( Sec-Websocket-Version for instance). Since App Gateway does not add WebSocket headers, the App Gateway's health probe response from your WebSocket server will most likely be 400 Bad Request . As a result App Gateway will mark your pods as unhealthy, which will eventually result in a 502 Bad Gateway for the consumers of the WebSocket server. To avoid this you may need to add an HTTP GET handler for a health check to your server ( /health for instance, which returns 200 OK ).","title":"Websockets"},{"location":"how-tos/websockets/#expose-a-websocket-server","text":"As outlined in the Application Gateway v2 documentation - it provides native support for the WebSocket and HTTP/2 protocols . Please note, that for both Application Gateway and the Kubernetes Ingress - there is no user-configurable setting to selectively enable or disable WebSocket support. The Kubernetes deployment YAML below shows the minimum configuration used to deploy a WebSocket server, which is the same as deploying a regular web server: apiVersion : apps/v1 kind : Deployment metadata : name : websocket-server spec : selector : matchLabels : app : ws-app replicas : 2 template : metadata : labels : app : ws-app spec : containers : - name : websocket-app imagePullPolicy : Always image : your-container-repo.azurecr.io/websockets-app ports : - containerPort : 8888 imagePullSecrets : - name : azure-container-registry-credentials --- apiVersion : v1 kind : Service metadata : name : websocket-app-service spec : selector : app : ws-app ports : - protocol : TCP port : 80 targetPort : 8888 --- apiVersion : extensions/v1beta1 kind : Ingress metadata : name : websocket-repeater annotations : kubernetes.io/ingress.class : azure/application-gateway spec : rules : - host : ws.contoso.com http : paths : - backend : serviceName : websocket-app-service servicePort : 80 Given that all the prerequisites are fulfilled, and you have an App Gateway controlled by a K8s Ingress in your AKS, the deployment above would result in a WebSockets server exposed on port 80 of your App Gateway's public IP and the ws.contoso.com domain. The following cURL command would test the WebSocket server deployment: curl -i -N -H \"Connection: Upgrade\" \\ -H \"Upgrade: websocket\" \\ -H \"Origin: http://localhost\" \\ -H \"Host: ws.contoso.com\" \\ -H \"Sec-Websocket-Version: 13\" \\ -H \"Sec-WebSocket-Key: 123\" \\ http://1.2.3.4:80/ws","title":"Expose a WebSocket server"},{"location":"how-tos/websockets/#websocket-health-probes","text":"If your deployment does not explicitly define health probes, App Gateway would attempt an HTTP GET on your WebSocket server endpoint. Depending on the server implementation ( here is one we love ) WebSocket specific headers may be required ( Sec-Websocket-Version for instance). Since App Gateway does not add WebSocket headers, the App Gateway's health probe response from your WebSocket server will most likely be 400 Bad Request . As a result App Gateway will mark your pods as unhealthy, which will eventually result in a 502 Bad Gateway for the consumers of the WebSocket server. To avoid this you may need to add an HTTP GET handler for a health check to your server ( /health for instance, which returns 200 OK ).","title":"WebSocket Health Probes"},{"location":"setup/install-existing/","text":"Brownfield Deployment The App Gateway Ingress Controller (AGIC) is a pod within your Kubernetes cluster. AGIC monitors the Kubernetes Ingress resources, and creates and applies App Gateway config based on these. Outline: Prerequisites Azure Resource Manager Authentication (ARM) Option 1: Set up aad-pod-identity and Create Azure Identity on ARM Option 2: Using a Service Principal Install Ingress Controller using Helm Multi-cluster / Shared App Gateway : Install AGIC in an environment, where App Gateway is shared between one or more AKS clusters and/or other Azure components. Prerequisites This documents assumes you already have the following tools and infrastructure installed: - AKS with Advanced Networking enabled - App Gateway v2 in the same virtual network as AKS - AAD Pod Identity installed on your AKS cluster - Cloud Shell is the Azure shell environment, which has az CLI, kubectl , and helm installed. These tools are required for the commands below. Please backup your App Gateway's configuration before installing AGIC: 1. using Azure Portal navigate to your App Gateway instance 2. from Export template click Download The zip file you downloaded will have JSON templates, bash, and PowerShell scripts you could use to restore App Gateway should that become necessary Install Helm Helm is a package manager for Kubernetes. We will leverage it to install the application-gateway-kubernetes-ingress package. Use Cloud Shell to install Helm: Install Helm and run the following to add application-gateway-kubernetes-ingress helm package: RBAC enabled AKS cluster kubectl create serviceaccount --namespace kube-system tiller-sa kubectl create clusterrolebinding tiller-cluster-rule --clusterrole = cluster-admin --serviceaccount = kube-system:tiller-sa helm init --tiller-namespace kube-system --service-account tiller-sa RBAC disabled AKS cluster helm init Add the AGIC Helm repository: helm repo add application-gateway-kubernetes-ingress https://appgwingress.blob.core.windows.net/ingress-azure-helm-package/ helm repo update Azure Resource Manager Authentication AGIC communicates with the Kubernetes API server and the Azure Resource Manager. It requires an identity to access these APIs. Set up AAD Pod Identity AAD Pod Identity is a controller, similar to AGIC, which also runs on your AKS. It binds Azure Active Directory identities to your Kubernetes pods. Identity is required for an application in a Kubernetes pod to be able to communicate with other Azure components. In the particular case here we need authorization for the AGIC pod to make HTTP requests to ARM . Follow the AAD Pod Identity installation instructions to add this component to your AKS. Next we need to create an Azure identity and give it permissions ARM. Use Cloud Shell to run all of the following commands and create an identity: Create an Azure identity in the same resource group as the AKS nodes . Picking the correct resource group is important. The resource group required in the command below is not the one referenced on the AKS portal pane. This is the resource group of the aks-agentpool virtual machines. Typically that resource group starts with MC_ and contains the name of your AKS. For instance: MC_resourceGroup_aksABCD_westus az identity create -g <agent-pool-resource-group> -n <identity-name> For the role assignment commands below we need to obtain clientId for the newly created identity: az identity show -g <resourcegroup> -n <identity-name> Give the identity Contributor access to you App Gateway. For this you need the ID of the App Gateway, which will look something like this: /subscriptions/A/resourceGroups/B/providers/Microsoft.Network/applicationGateways/C Get the list of App Gateway IDs in your subscription with: az network application-gateway list --query '[].id' az role assignment create \\ --role Contributor \\ --assignee <clientId> \\ --scope <App-Gateway-ID> Give the identity Reader access to the App Gateway resource group. The resource group ID would look like: /subscriptions/A/resourceGroups/B . You can get all resource groups with: az group list --query '[].id' az role assignment create \\ --role Reader \\ --assignee <clientId> \\ --scope <App-Gateway-Resource-Group-ID> Using a Service Principal It is also possible to provide AGIC access to ARM via a Kubernetes secret. Create an Active Directory Service Principal and encode with base64. The base64 encoding is required for the JSON blob to be saved to Kubernetes. az ad sp create-for-rbac --sdk-auth | base64 -w0 Add the base64 encoded JSON blob to the helm-config.yaml file. More information on helm-config.yaml is in the next section. armAuth : type : servicePrincipal secretJSON : <Base64-Encoded-Credentials> Install Ingress Controller as a Helm Chart In the first few steps we install Helm's Tiller on your Kubernetes cluster. Use Cloud Shell to install the AGIC Helm package: Add the application-gateway-kubernetes-ingress helm repo and perform a helm update helm repo add application-gateway-kubernetes-ingress https://appgwingress.blob.core.windows.net/ingress-azure-helm-package/ helm repo update Download helm-config.yaml , which will configure AGIC: wget https://raw.githubusercontent.com/Azure/application-gateway-kubernetes-ingress/master/docs/examples/sample-helm-config.yaml -O helm-config.yaml Edit helm-config.yaml and fill in the values for appgw and armAuth . nano helm-config.yaml NOTE: The <identity-resource-id> and <identity-client-id> are the properties of the Azure AD Identity you setup in the previous section. You can retrieve this information by running the following command: az identity show -g <resourcegroup> -n <identity-name> , where <resourcegroup> is the resource group in which the top level AKS cluster object, Application Gateway and Managed Identify are deployed. Install Helm chart application-gateway-kubernetes-ingress with the helm-config.yaml configuration from the previous step helm install -f <helm-config.yaml> application-gateway-kubernetes-ingress/ingress-azure Alternatively you can combine the helm-config.yaml and the Helm command in one step: helm install ./helm/ingress-azure \\ --name ingress-azure \\ --namespace default \\ --debug \\ --set appgw.name = applicationgatewayABCD \\ --set appgw.resourceGroup = your-resource-group \\ --set appgw.subscriptionId = subscription-uuid \\ --set appgw.shared = false \\ --set armAuth.type = servicePrincipal \\ --set armAuth.secretJSON = $( az ad sp create-for-rbac --sdk-auth | base64 -w0 ) \\ --set rbac.enabled = true \\ --set verbosityLevel = 3 \\ --set kubernetes.watchNamespace = default \\ --set aksClusterConfiguration.apiServerAddress = aks-abcdefg.hcp.westus2.azmk8s.io Check the log of the newly created pod to verify if it started properly Refer to the tutorials to understand how you can expose an AKS service over HTTP or HTTPS, to the internet, using an Azure App Gateway. Multi-cluster / Shared App Gateway By default AGIC assumes full ownership of the App Gateway it is linked to. AGIC version 0.8.0 and later can share a single App Gateway with other Azure components. For instance, we could use the same App Gateway for an app hosted on VMSS as well as an AKS cluster. Please backup your App Gateway's configuration before enabling this setting: 1. using Azure Portal navigate to your App Gateway instance 2. from Export template click Download The zip file you downloaded will have JSON templates, bash, and PowerShell scripts you could use to restore App Gateway Example Scenario Let's look at an imaginary App Gateway, which manages traffic for 2 web sites: - dev.contoso.com - hosted on a new AKS, using App Gateway and AGIC - prod.contoso.com - hosted on an Azure VMSS With default settings, AGIC assumes 100% ownership of the App Gateway it is pointed to. AGIC overwrites all of App Gateway's configuration. If we were to manually create a listener for prod.contoso.com (on App Gateway), without defining it in the Kubernetes Ingress, AGIC will delete the prod.contoso.com config within seconds. To install AGIC and also serve prod.contoso.com from our VMSS machines, we must constrain AGIC to configuring dev.contoso.com only. This is facilitated by instantiating the following CRD : cat <<EOF | kubectl apply -f - apiVersion: \"appgw.ingress.k8s.io/v1\" kind: AzureIngressProhibitedTarget metadata: name: prod-contoso-com spec: hostname: prod.contoso.com EOF The command above creates an AzureIngressProhibitedTarget object. This makes AGIC (version 0.8.0 and later) aware of the existence of App Gateway config for prod.contoso.com and explicitly instructs it to avoid changing any configuration related to that hostname. Enable with new AGIC installation To limit AGIC (version 0.8.0 and later) to a subset of the App Gateway configuration modify the helm-config.yaml template. Under the appgw: section, add shared key and set it to to true . appgw : subscriptionId : <subscriptionId> # existing field resourceGroup : <resourceGroupName> # existing field name : <applicationGatewayName> # existing field shared : true # <<<<< Add this field to enable shared App Gateway >>>>> Apply the Helm changes: 1. Ensure the AzureIngressProhibitedTarget CRD is installed with: kubectl apply -f https://raw.githubusercontent.com/Azure/application-gateway-kubernetes-ingress/ae695ef9bd05c8b708cedf6ff545595d0b7022dc/crds/AzureIngressProhibitedTarget.yaml 2. Update Helm: helm upgrade \\ --recreate-pods \\ -f helm-config.yaml \\ ingress-azure application-gateway-kubernetes-ingress/ingress-azure As a result your AKS will have a new instance of AzureIngressProhibitedTarget called prohibit-all-targets : kubectl get AzureIngressProhibitedTargets prohibit-all-targets -o yaml The object prohibit-all-targets , as the name implies, prohibits AGIC from changing config for any host and path. Helm install with appgw.shared=true will deploy AGIC, but will not make any changes to App Gateway. Broaden permissions Since Helm with appgw.shared=true and the default prohibit-all-targets blocks AGIC from applying any config. Broaden AGIC permissions with: 1. Create a new AzureIngressProhibitedTarget with your specific setup: cat <<EOF | kubectl apply -f - apiVersion: \"appgw.ingress.k8s.io/v1\" kind: AzureIngressProhibitedTarget metadata: name: your-custom-prohibitions spec: hostname: your.own-hostname.com EOF Only after you have created your own custom prohibition, you can delete the default one, which is too broad: kubectl delete AzureIngressProhibitedTarget prohibit-all-targets Enable for an existing AGIC installation Let's assume that we already have a working AKS, App Gateway, and configured AGIC in our cluster. We have an Ingress for prod.contosor.com and are successfully serving traffic for it from AKS. We want to add staging.contoso.com to our existing App Gateway, but need to host it on a VM . We are going to re-use the existing App Gateway and manually configure a listener and backend pools for staging.contoso.com . But manually tweaking App Gateway config (via portal , ARM APIs or Terraform ) would conflict with AGIC's assumptions of full ownership. Shortly after we apply changes, AGIC will overwrite or delete them. We can prohibit AGIC from making changes to a subset of configuration. Create an AzureIngressProhibitedTarget object: cat <<EOF | kubectl apply -f - apiVersion: \"appgw.ingress.k8s.io/v1\" kind: AzureIngressProhibitedTarget metadata: name: manually-configured-staging-environment spec: hostname: staging.contoso.com EOF View the newly created object: kubectl get AzureIngressProhibitedTargets Modify App Gateway config via portal - add listeners, routing rules, backends etc. The new object we created ( manually-configured-staging-environment ) will prohibit AGIC from overwriting App Gateway configuration related to staging.contoso.com .","title":"Brownfield Deployment"},{"location":"setup/install-existing/#brownfield-deployment","text":"The App Gateway Ingress Controller (AGIC) is a pod within your Kubernetes cluster. AGIC monitors the Kubernetes Ingress resources, and creates and applies App Gateway config based on these.","title":"Brownfield Deployment"},{"location":"setup/install-existing/#outline","text":"Prerequisites Azure Resource Manager Authentication (ARM) Option 1: Set up aad-pod-identity and Create Azure Identity on ARM Option 2: Using a Service Principal Install Ingress Controller using Helm Multi-cluster / Shared App Gateway : Install AGIC in an environment, where App Gateway is shared between one or more AKS clusters and/or other Azure components.","title":"Outline:"},{"location":"setup/install-existing/#prerequisites","text":"This documents assumes you already have the following tools and infrastructure installed: - AKS with Advanced Networking enabled - App Gateway v2 in the same virtual network as AKS - AAD Pod Identity installed on your AKS cluster - Cloud Shell is the Azure shell environment, which has az CLI, kubectl , and helm installed. These tools are required for the commands below. Please backup your App Gateway's configuration before installing AGIC: 1. using Azure Portal navigate to your App Gateway instance 2. from Export template click Download The zip file you downloaded will have JSON templates, bash, and PowerShell scripts you could use to restore App Gateway should that become necessary","title":"Prerequisites"},{"location":"setup/install-existing/#install-helm","text":"Helm is a package manager for Kubernetes. We will leverage it to install the application-gateway-kubernetes-ingress package. Use Cloud Shell to install Helm: Install Helm and run the following to add application-gateway-kubernetes-ingress helm package: RBAC enabled AKS cluster kubectl create serviceaccount --namespace kube-system tiller-sa kubectl create clusterrolebinding tiller-cluster-rule --clusterrole = cluster-admin --serviceaccount = kube-system:tiller-sa helm init --tiller-namespace kube-system --service-account tiller-sa RBAC disabled AKS cluster helm init Add the AGIC Helm repository: helm repo add application-gateway-kubernetes-ingress https://appgwingress.blob.core.windows.net/ingress-azure-helm-package/ helm repo update","title":"Install Helm"},{"location":"setup/install-existing/#azure-resource-manager-authentication","text":"AGIC communicates with the Kubernetes API server and the Azure Resource Manager. It requires an identity to access these APIs.","title":"Azure Resource Manager Authentication"},{"location":"setup/install-existing/#set-up-aad-pod-identity","text":"AAD Pod Identity is a controller, similar to AGIC, which also runs on your AKS. It binds Azure Active Directory identities to your Kubernetes pods. Identity is required for an application in a Kubernetes pod to be able to communicate with other Azure components. In the particular case here we need authorization for the AGIC pod to make HTTP requests to ARM . Follow the AAD Pod Identity installation instructions to add this component to your AKS. Next we need to create an Azure identity and give it permissions ARM. Use Cloud Shell to run all of the following commands and create an identity: Create an Azure identity in the same resource group as the AKS nodes . Picking the correct resource group is important. The resource group required in the command below is not the one referenced on the AKS portal pane. This is the resource group of the aks-agentpool virtual machines. Typically that resource group starts with MC_ and contains the name of your AKS. For instance: MC_resourceGroup_aksABCD_westus az identity create -g <agent-pool-resource-group> -n <identity-name> For the role assignment commands below we need to obtain clientId for the newly created identity: az identity show -g <resourcegroup> -n <identity-name> Give the identity Contributor access to you App Gateway. For this you need the ID of the App Gateway, which will look something like this: /subscriptions/A/resourceGroups/B/providers/Microsoft.Network/applicationGateways/C Get the list of App Gateway IDs in your subscription with: az network application-gateway list --query '[].id' az role assignment create \\ --role Contributor \\ --assignee <clientId> \\ --scope <App-Gateway-ID> Give the identity Reader access to the App Gateway resource group. The resource group ID would look like: /subscriptions/A/resourceGroups/B . You can get all resource groups with: az group list --query '[].id' az role assignment create \\ --role Reader \\ --assignee <clientId> \\ --scope <App-Gateway-Resource-Group-ID>","title":"Set up AAD Pod Identity"},{"location":"setup/install-existing/#using-a-service-principal","text":"It is also possible to provide AGIC access to ARM via a Kubernetes secret. Create an Active Directory Service Principal and encode with base64. The base64 encoding is required for the JSON blob to be saved to Kubernetes. az ad sp create-for-rbac --sdk-auth | base64 -w0 Add the base64 encoded JSON blob to the helm-config.yaml file. More information on helm-config.yaml is in the next section. armAuth : type : servicePrincipal secretJSON : <Base64-Encoded-Credentials>","title":"Using a Service Principal"},{"location":"setup/install-existing/#install-ingress-controller-as-a-helm-chart","text":"In the first few steps we install Helm's Tiller on your Kubernetes cluster. Use Cloud Shell to install the AGIC Helm package: Add the application-gateway-kubernetes-ingress helm repo and perform a helm update helm repo add application-gateway-kubernetes-ingress https://appgwingress.blob.core.windows.net/ingress-azure-helm-package/ helm repo update Download helm-config.yaml , which will configure AGIC: wget https://raw.githubusercontent.com/Azure/application-gateway-kubernetes-ingress/master/docs/examples/sample-helm-config.yaml -O helm-config.yaml Edit helm-config.yaml and fill in the values for appgw and armAuth . nano helm-config.yaml NOTE: The <identity-resource-id> and <identity-client-id> are the properties of the Azure AD Identity you setup in the previous section. You can retrieve this information by running the following command: az identity show -g <resourcegroup> -n <identity-name> , where <resourcegroup> is the resource group in which the top level AKS cluster object, Application Gateway and Managed Identify are deployed. Install Helm chart application-gateway-kubernetes-ingress with the helm-config.yaml configuration from the previous step helm install -f <helm-config.yaml> application-gateway-kubernetes-ingress/ingress-azure Alternatively you can combine the helm-config.yaml and the Helm command in one step: helm install ./helm/ingress-azure \\ --name ingress-azure \\ --namespace default \\ --debug \\ --set appgw.name = applicationgatewayABCD \\ --set appgw.resourceGroup = your-resource-group \\ --set appgw.subscriptionId = subscription-uuid \\ --set appgw.shared = false \\ --set armAuth.type = servicePrincipal \\ --set armAuth.secretJSON = $( az ad sp create-for-rbac --sdk-auth | base64 -w0 ) \\ --set rbac.enabled = true \\ --set verbosityLevel = 3 \\ --set kubernetes.watchNamespace = default \\ --set aksClusterConfiguration.apiServerAddress = aks-abcdefg.hcp.westus2.azmk8s.io Check the log of the newly created pod to verify if it started properly Refer to the tutorials to understand how you can expose an AKS service over HTTP or HTTPS, to the internet, using an Azure App Gateway.","title":"Install Ingress Controller as a Helm Chart"},{"location":"setup/install-existing/#multi-cluster-shared-app-gateway","text":"By default AGIC assumes full ownership of the App Gateway it is linked to. AGIC version 0.8.0 and later can share a single App Gateway with other Azure components. For instance, we could use the same App Gateway for an app hosted on VMSS as well as an AKS cluster. Please backup your App Gateway's configuration before enabling this setting: 1. using Azure Portal navigate to your App Gateway instance 2. from Export template click Download The zip file you downloaded will have JSON templates, bash, and PowerShell scripts you could use to restore App Gateway","title":"Multi-cluster / Shared App Gateway"},{"location":"setup/install-existing/#example-scenario","text":"Let's look at an imaginary App Gateway, which manages traffic for 2 web sites: - dev.contoso.com - hosted on a new AKS, using App Gateway and AGIC - prod.contoso.com - hosted on an Azure VMSS With default settings, AGIC assumes 100% ownership of the App Gateway it is pointed to. AGIC overwrites all of App Gateway's configuration. If we were to manually create a listener for prod.contoso.com (on App Gateway), without defining it in the Kubernetes Ingress, AGIC will delete the prod.contoso.com config within seconds. To install AGIC and also serve prod.contoso.com from our VMSS machines, we must constrain AGIC to configuring dev.contoso.com only. This is facilitated by instantiating the following CRD : cat <<EOF | kubectl apply -f - apiVersion: \"appgw.ingress.k8s.io/v1\" kind: AzureIngressProhibitedTarget metadata: name: prod-contoso-com spec: hostname: prod.contoso.com EOF The command above creates an AzureIngressProhibitedTarget object. This makes AGIC (version 0.8.0 and later) aware of the existence of App Gateway config for prod.contoso.com and explicitly instructs it to avoid changing any configuration related to that hostname.","title":"Example Scenario"},{"location":"setup/install-existing/#enable-with-new-agic-installation","text":"To limit AGIC (version 0.8.0 and later) to a subset of the App Gateway configuration modify the helm-config.yaml template. Under the appgw: section, add shared key and set it to to true . appgw : subscriptionId : <subscriptionId> # existing field resourceGroup : <resourceGroupName> # existing field name : <applicationGatewayName> # existing field shared : true # <<<<< Add this field to enable shared App Gateway >>>>> Apply the Helm changes: 1. Ensure the AzureIngressProhibitedTarget CRD is installed with: kubectl apply -f https://raw.githubusercontent.com/Azure/application-gateway-kubernetes-ingress/ae695ef9bd05c8b708cedf6ff545595d0b7022dc/crds/AzureIngressProhibitedTarget.yaml 2. Update Helm: helm upgrade \\ --recreate-pods \\ -f helm-config.yaml \\ ingress-azure application-gateway-kubernetes-ingress/ingress-azure As a result your AKS will have a new instance of AzureIngressProhibitedTarget called prohibit-all-targets : kubectl get AzureIngressProhibitedTargets prohibit-all-targets -o yaml The object prohibit-all-targets , as the name implies, prohibits AGIC from changing config for any host and path. Helm install with appgw.shared=true will deploy AGIC, but will not make any changes to App Gateway.","title":"Enable with new AGIC installation"},{"location":"setup/install-existing/#broaden-permissions","text":"Since Helm with appgw.shared=true and the default prohibit-all-targets blocks AGIC from applying any config. Broaden AGIC permissions with: 1. Create a new AzureIngressProhibitedTarget with your specific setup: cat <<EOF | kubectl apply -f - apiVersion: \"appgw.ingress.k8s.io/v1\" kind: AzureIngressProhibitedTarget metadata: name: your-custom-prohibitions spec: hostname: your.own-hostname.com EOF Only after you have created your own custom prohibition, you can delete the default one, which is too broad: kubectl delete AzureIngressProhibitedTarget prohibit-all-targets","title":"Broaden permissions"},{"location":"setup/install-existing/#enable-for-an-existing-agic-installation","text":"Let's assume that we already have a working AKS, App Gateway, and configured AGIC in our cluster. We have an Ingress for prod.contosor.com and are successfully serving traffic for it from AKS. We want to add staging.contoso.com to our existing App Gateway, but need to host it on a VM . We are going to re-use the existing App Gateway and manually configure a listener and backend pools for staging.contoso.com . But manually tweaking App Gateway config (via portal , ARM APIs or Terraform ) would conflict with AGIC's assumptions of full ownership. Shortly after we apply changes, AGIC will overwrite or delete them. We can prohibit AGIC from making changes to a subset of configuration. Create an AzureIngressProhibitedTarget object: cat <<EOF | kubectl apply -f - apiVersion: \"appgw.ingress.k8s.io/v1\" kind: AzureIngressProhibitedTarget metadata: name: manually-configured-staging-environment spec: hostname: staging.contoso.com EOF View the newly created object: kubectl get AzureIngressProhibitedTargets Modify App Gateway config via portal - add listeners, routing rules, backends etc. The new object we created ( manually-configured-staging-environment ) will prohibit AGIC from overwriting App Gateway configuration related to staging.contoso.com .","title":"Enable for an existing AGIC installation"},{"location":"setup/install-new/","text":"Greenfield Deployment The instructions below assume Application Gateway Ingress Controller (AGIC) will be installed in an environment with no pre-existing components. Required Command Line Tools We recommend the use of Azure Cloud Shell for all command line operations below. Launch your shell from shell.azure.com or by clicking the link: Alternatively, launch Cloud Shell from Azure portal using the following icon: Your Azure Cloud Shell already has all necessary tools. Should you choose to use another environment, please ensure the following command line tools are installed: az - Azure CLI: installation instructions kubectl - Kubernetes command-line tool: installation instructions helm - Kubernetes package manager: installation instructions jq - command-line JSON processor: installation instructions Create an Identity Follow the steps below to create an Azure Active Directory (AAD) service principal object . Please record the appId , password , and objectId values - these will be used in the following steps. Create AD service principal ( Read more about RBAC ). Paste the following lines in your Azure Cloud Shell : az ad sp create-for-rbac --skip-assignment -o json > auth.json appId = $( jq -r \".appId\" auth.json ) password = $( jq -r \".password\" auth.json ) These commands will create appId and password bash variables, which will be used in the steps below. You can view the value of these with echo $appId and echo $password . Execute the next command in Cloud Shell to create the objectId bash variable, which is the new Service Princpial: objectId = $( az ad sp show --id $appId --query \"objectId\" -o tsv ) The objectId bash variable will be used in the ARM template below. View the value with echo $objectId . Paste the entire command below (it is a single command on multiple lines) in Cloud Shell to create the parameters.json file. It will be used in the ARM template deployment. cat <<EOF > parameters.json { \"aksServicePrincipalAppId\": { \"value\": \"$appId\" }, \"aksServicePrincipalClientSecret\": { \"value\": \"$password\" }, \"aksServicePrincipalObjectId\": { \"value\": \"$objectId\" }, \"aksEnableRBAC\": { \"value\": false } } EOF To deploy an RBAC enabled cluster, set the aksEnabledRBAC field to true . View the contents of the newly created file with cat parameters.json . It will contain the values of the appId , password , and objectId bash variables from the previous steps. Deploy Components The next few steps will add the following list of components to your Azure subscription: Azure Kubernetes Service Application Gateway v2 Virtual Network with 2 subnets Public IP Address Managed Identity , which will be used by AAD Pod Identity Download the ARM template into template.json file. Paste the following in your shell : wget https://raw.githubusercontent.com/Azure/application-gateway-kubernetes-ingress/master/deploy/azuredeploy.json -O template.json Deploy the ARM template via Azure Cloud Shell and the az tool. Modify the name of the resource group and region/location, then paste each of the following lines into your shell : resourceGroupName = \"MyResourceGroup\" location = \"westus2\" deploymentName = \"ingress-appgw\" az group create -n $resourceGroupName -l $location az group deployment create -g $resourceGroupName -n $deploymentName --template-file template.json --parameters parameters.json Note: The last command may take a few minutes to complete. Once the deployment finished, download the deployment output into a file named deployment-outputs.json . az group deployment show -g $resourceGroupName -n $deploymentName --query \"properties.outputs\" -o json > deployment-outputs.json View the content of the newly created file with: cat deployment-outputs.json . The file will have the following shape (example): { \"aksApiServerAddress\" : { \"type\" : \"String\" , \"value\" : \"aks-abcd41e9.hcp.westus2.azmk8s.io\" }, \"aksClusterName\" : { \"type\" : \"String\" , \"value\" : \"aksabcd\" }, \"applicationGatewayName\" : { \"type\" : \"String\" , \"value\" : \"applicationgatewayabcd\" }, \"identityClientId\" : { \"type\" : \"String\" , \"value\" : \"7b1a3378-8abe-ab58-cca9-a8ef624db293\" }, \"identityResourceId\" : { \"type\" : \"String\" , \"value\" : \"/subscriptions/a6466a81-bf0d-147e-2acb-a0ba50f6456e/resourceGroups/MyResourceGroup/providers/Microsoft.ManagedIdentity/userAssignedIdentities/appgwContrIdentityabcd\" }, \"resourceGroupName\" : { \"type\" : \"String\" , \"value\" : \"MyResourceGroup\" }, \"subscriptionId\" : { \"type\" : \"String\" , \"value\" : \"a6466a81-bf0d-147e-2acb-a0ba50f6456e\" } } Set up Application Gateway Ingress Controller With the instructions in the previous section we created and configured a new AKS cluster and an App Gateway. We are now ready to deploy a sample app and an ingress controller to our new Kubernetes infrastructure. Setup Kubernetes Credentials For the following steps we need setup kubectl command, which we will use to connect to our new Kubernetes cluster. Cloud Shell has kubectl already installed. We will use az CLI to obtain credentials for Kubernetes. Get credentials for your newly deployed AKS ( read more ): # use the deployment-outputs.json created after deployment to get the cluster name and resource group name aksClusterName = $( jq -r \".aksClusterName.value\" deployment-outputs.json ) resourceGroupName = $( jq -r \".resourceGroupName.value\" deployment-outputs.json ) az aks get-credentials --resource-group $resourceGroupName --name $aksClusterName Install AAD Pod Identity Azure Active Directory Pod Identity provides token-based access to Azure Resource Manager (ARM) . AAD Pod Identity will add the following components to your Kubernetes cluster: 1. Kubernetes CRDs : AzureIdentity , AzureAssignedIdentity , AzureIdentityBinding 1. Managed Identity Controller (MIC) component 1. Node Managed Identity (NMI) component To install AAD Pod Identity to your cluster: RBAC enabled AKS cluster kubectl create -f https://raw.githubusercontent.com/Azure/aad-pod-identity/master/deploy/infra/deployment-rbac.yaml RBAC disabled AKS cluster kubectl create -f https://raw.githubusercontent.com/Azure/aad-pod-identity/master/deploy/infra/deployment.yaml Install Helm Helm is a package manager for Kubernetes. We will leverage it to install the application-gateway-kubernetes-ingress package: Install Helm and run the following to add application-gateway-kubernetes-ingress helm package: RBAC enabled AKS cluster kubectl create serviceaccount --namespace kube-system tiller-sa kubectl create clusterrolebinding tiller-cluster-rule --clusterrole = cluster-admin --serviceaccount = kube-system:tiller-sa helm init --tiller-namespace kube-system --service-account tiller-sa RBAC disabled AKS cluster helm init Add the AGIC Helm repository: helm repo add application-gateway-kubernetes-ingress https://appgwingress.blob.core.windows.net/ingress-azure-helm-package/ helm repo update Install Ingress Controller Helm Chart Use the deployment-outputs.json file created above and create the following variables. applicationGatewayName = $( jq -r \".applicationGatewayName.value\" deployment-outputs.json ) resourceGroupName = $( jq -r \".resourceGroupName.value\" deployment-outputs.json ) subscriptionId = $( jq -r \".subscriptionId.value\" deployment-outputs.json ) identityClientId = $( jq -r \".identityClientId.value\" deployment-outputs.json ) identityResourceId = $( jq -r \".identityResourceId.value\" deployment-outputs.json ) Download helm-config.yaml , which will configure AGIC: wget https://raw.githubusercontent.com/Azure/application-gateway-kubernetes-ingress/master/docs/examples/sample-helm-config.yaml -O helm-config.yaml Edit the newly downloaded helm-config.yaml and fill out the sections appgw and armAuth . sed -i \"s|<subscriptionId>| ${ subscriptionId } |g\" helm-config.yaml sed -i \"s|<resourceGroupName>| ${ resourceGroupName } |g\" helm-config.yaml sed -i \"s|<applicationGatewayName>| ${ applicationGatewayName } |g\" helm-config.yaml sed -i \"s|<identityResourceId>| ${ identityResourceId } |g\" helm-config.yaml sed -i \"s|<identityClientId>| ${ identityClientId } |g\" helm-config.yaml # You can further modify the helm config to enable/disable features nano helm-config.yaml Values: - verbosityLevel : Sets the verbosity level of the AGIC logging infrastructure. See Logging Levels for possible values. - appgw.subscriptionId : The Azure Subscription ID in which App Gateway resides. Example: a123b234-a3b4-557d-b2df-a0bc12de1234 - appgw.resourceGroup : Name of the Azure Resource Group in which App Gateway was created. Example: app-gw-resource-group - appgw.name : Name of the Application Gateway. Example: applicationgatewayd0f0 - appgw.shared : This boolean flag should be defaulted to false . Set to true should you need a Shared App Gateway . - kubernetes.watchNamespace : Specify the name space, which AGIC should watch. This could be a single string value, or a comma-separated list of namespaces. - armAuth.type : could be aadPodIdentity or servicePrincipal - armAuth.identityResourceID : Resource ID of the Azure Managed Identity - armAuth.identityClientId : The Client ID of the Identity. See below for more information on Identity - armAuth.secretJSON : Only needed when Service Principal Secret type is chosen (when armAuth.type has been set to servicePrincipal ) - rbac.enabled : Make sure to set this to true if you have a AKS cluster that is RBAC enabled. Note on Identity: The identityResourceID and identityClientID are values that were created during the Create an Identity steps, and could be obtained again using the following command: az identity show -g <resource-group> -n <identity-name> - <resource-group> in the command above is the resource group of your App Gateway. - <identity-name> is the name of the created identity. All identities for a given subscription can be listed using: az identity list Install the Application Gateway ingress controller package: helm install -f helm-config.yaml application-gateway-kubernetes-ingress/ingress-azure Install a Sample App Now that we have App Gateway, AKS, and AGIC installed we can install a sample app via Azure Cloud Shell : cat <<EOF | kubectl apply -f - apiVersion : v1 kind : Pod metadata : name : aspnetapp labels : app : aspnetapp spec : containers : - image : \"mcr.microsoft.com/dotnet/core/samples:aspnetapp\" name : aspnetapp-image ports : - containerPort : 80 protocol : TCP --- apiVersion : v1 kind : Service metadata : name : aspnetapp spec : selector : app : aspnetapp ports : - protocol : TCP port : 80 targetPort : 80 --- apiVersion : extensions/v1beta1 kind : Ingress metadata : name : aspnetapp annotations : kubernetes.io/ingress.class : azure/application-gateway spec : rules : - http : paths : - path : / backend : serviceName : aspnetapp servicePort : 80 EOF Alternatively you can: Download the YAML file above: curl https://raw.githubusercontent.com/Azure/application-gateway-kubernetes-ingress/master/docs/examples/aspnetapp.yaml -o aspnetapp.yaml Apply the YAML file: kubectl apply -f aspnetapp.yaml Other Examples The tutorials document contains more examples on how toexpose an AKS service via HTTP or HTTPS, to the Internet with App Gateway.","title":"Greenfield Deployment"},{"location":"setup/install-new/#greenfield-deployment","text":"The instructions below assume Application Gateway Ingress Controller (AGIC) will be installed in an environment with no pre-existing components.","title":"Greenfield Deployment"},{"location":"setup/install-new/#required-command-line-tools","text":"We recommend the use of Azure Cloud Shell for all command line operations below. Launch your shell from shell.azure.com or by clicking the link: Alternatively, launch Cloud Shell from Azure portal using the following icon: Your Azure Cloud Shell already has all necessary tools. Should you choose to use another environment, please ensure the following command line tools are installed: az - Azure CLI: installation instructions kubectl - Kubernetes command-line tool: installation instructions helm - Kubernetes package manager: installation instructions jq - command-line JSON processor: installation instructions","title":"Required Command Line Tools"},{"location":"setup/install-new/#create-an-identity","text":"Follow the steps below to create an Azure Active Directory (AAD) service principal object . Please record the appId , password , and objectId values - these will be used in the following steps. Create AD service principal ( Read more about RBAC ). Paste the following lines in your Azure Cloud Shell : az ad sp create-for-rbac --skip-assignment -o json > auth.json appId = $( jq -r \".appId\" auth.json ) password = $( jq -r \".password\" auth.json ) These commands will create appId and password bash variables, which will be used in the steps below. You can view the value of these with echo $appId and echo $password . Execute the next command in Cloud Shell to create the objectId bash variable, which is the new Service Princpial: objectId = $( az ad sp show --id $appId --query \"objectId\" -o tsv ) The objectId bash variable will be used in the ARM template below. View the value with echo $objectId . Paste the entire command below (it is a single command on multiple lines) in Cloud Shell to create the parameters.json file. It will be used in the ARM template deployment. cat <<EOF > parameters.json { \"aksServicePrincipalAppId\": { \"value\": \"$appId\" }, \"aksServicePrincipalClientSecret\": { \"value\": \"$password\" }, \"aksServicePrincipalObjectId\": { \"value\": \"$objectId\" }, \"aksEnableRBAC\": { \"value\": false } } EOF To deploy an RBAC enabled cluster, set the aksEnabledRBAC field to true . View the contents of the newly created file with cat parameters.json . It will contain the values of the appId , password , and objectId bash variables from the previous steps.","title":"Create an Identity"},{"location":"setup/install-new/#deploy-components","text":"The next few steps will add the following list of components to your Azure subscription: Azure Kubernetes Service Application Gateway v2 Virtual Network with 2 subnets Public IP Address Managed Identity , which will be used by AAD Pod Identity Download the ARM template into template.json file. Paste the following in your shell : wget https://raw.githubusercontent.com/Azure/application-gateway-kubernetes-ingress/master/deploy/azuredeploy.json -O template.json Deploy the ARM template via Azure Cloud Shell and the az tool. Modify the name of the resource group and region/location, then paste each of the following lines into your shell : resourceGroupName = \"MyResourceGroup\" location = \"westus2\" deploymentName = \"ingress-appgw\" az group create -n $resourceGroupName -l $location az group deployment create -g $resourceGroupName -n $deploymentName --template-file template.json --parameters parameters.json Note: The last command may take a few minutes to complete. Once the deployment finished, download the deployment output into a file named deployment-outputs.json . az group deployment show -g $resourceGroupName -n $deploymentName --query \"properties.outputs\" -o json > deployment-outputs.json View the content of the newly created file with: cat deployment-outputs.json . The file will have the following shape (example): { \"aksApiServerAddress\" : { \"type\" : \"String\" , \"value\" : \"aks-abcd41e9.hcp.westus2.azmk8s.io\" }, \"aksClusterName\" : { \"type\" : \"String\" , \"value\" : \"aksabcd\" }, \"applicationGatewayName\" : { \"type\" : \"String\" , \"value\" : \"applicationgatewayabcd\" }, \"identityClientId\" : { \"type\" : \"String\" , \"value\" : \"7b1a3378-8abe-ab58-cca9-a8ef624db293\" }, \"identityResourceId\" : { \"type\" : \"String\" , \"value\" : \"/subscriptions/a6466a81-bf0d-147e-2acb-a0ba50f6456e/resourceGroups/MyResourceGroup/providers/Microsoft.ManagedIdentity/userAssignedIdentities/appgwContrIdentityabcd\" }, \"resourceGroupName\" : { \"type\" : \"String\" , \"value\" : \"MyResourceGroup\" }, \"subscriptionId\" : { \"type\" : \"String\" , \"value\" : \"a6466a81-bf0d-147e-2acb-a0ba50f6456e\" } }","title":"Deploy Components"},{"location":"setup/install-new/#set-up-application-gateway-ingress-controller","text":"With the instructions in the previous section we created and configured a new AKS cluster and an App Gateway. We are now ready to deploy a sample app and an ingress controller to our new Kubernetes infrastructure.","title":"Set up Application Gateway Ingress Controller"},{"location":"setup/install-new/#setup-kubernetes-credentials","text":"For the following steps we need setup kubectl command, which we will use to connect to our new Kubernetes cluster. Cloud Shell has kubectl already installed. We will use az CLI to obtain credentials for Kubernetes. Get credentials for your newly deployed AKS ( read more ): # use the deployment-outputs.json created after deployment to get the cluster name and resource group name aksClusterName = $( jq -r \".aksClusterName.value\" deployment-outputs.json ) resourceGroupName = $( jq -r \".resourceGroupName.value\" deployment-outputs.json ) az aks get-credentials --resource-group $resourceGroupName --name $aksClusterName","title":"Setup Kubernetes Credentials"},{"location":"setup/install-new/#install-aad-pod-identity","text":"Azure Active Directory Pod Identity provides token-based access to Azure Resource Manager (ARM) . AAD Pod Identity will add the following components to your Kubernetes cluster: 1. Kubernetes CRDs : AzureIdentity , AzureAssignedIdentity , AzureIdentityBinding 1. Managed Identity Controller (MIC) component 1. Node Managed Identity (NMI) component To install AAD Pod Identity to your cluster: RBAC enabled AKS cluster kubectl create -f https://raw.githubusercontent.com/Azure/aad-pod-identity/master/deploy/infra/deployment-rbac.yaml RBAC disabled AKS cluster kubectl create -f https://raw.githubusercontent.com/Azure/aad-pod-identity/master/deploy/infra/deployment.yaml","title":"Install AAD Pod Identity"},{"location":"setup/install-new/#install-helm","text":"Helm is a package manager for Kubernetes. We will leverage it to install the application-gateway-kubernetes-ingress package: Install Helm and run the following to add application-gateway-kubernetes-ingress helm package: RBAC enabled AKS cluster kubectl create serviceaccount --namespace kube-system tiller-sa kubectl create clusterrolebinding tiller-cluster-rule --clusterrole = cluster-admin --serviceaccount = kube-system:tiller-sa helm init --tiller-namespace kube-system --service-account tiller-sa RBAC disabled AKS cluster helm init Add the AGIC Helm repository: helm repo add application-gateway-kubernetes-ingress https://appgwingress.blob.core.windows.net/ingress-azure-helm-package/ helm repo update","title":"Install Helm"},{"location":"setup/install-new/#install-ingress-controller-helm-chart","text":"Use the deployment-outputs.json file created above and create the following variables. applicationGatewayName = $( jq -r \".applicationGatewayName.value\" deployment-outputs.json ) resourceGroupName = $( jq -r \".resourceGroupName.value\" deployment-outputs.json ) subscriptionId = $( jq -r \".subscriptionId.value\" deployment-outputs.json ) identityClientId = $( jq -r \".identityClientId.value\" deployment-outputs.json ) identityResourceId = $( jq -r \".identityResourceId.value\" deployment-outputs.json ) Download helm-config.yaml , which will configure AGIC: wget https://raw.githubusercontent.com/Azure/application-gateway-kubernetes-ingress/master/docs/examples/sample-helm-config.yaml -O helm-config.yaml Edit the newly downloaded helm-config.yaml and fill out the sections appgw and armAuth . sed -i \"s|<subscriptionId>| ${ subscriptionId } |g\" helm-config.yaml sed -i \"s|<resourceGroupName>| ${ resourceGroupName } |g\" helm-config.yaml sed -i \"s|<applicationGatewayName>| ${ applicationGatewayName } |g\" helm-config.yaml sed -i \"s|<identityResourceId>| ${ identityResourceId } |g\" helm-config.yaml sed -i \"s|<identityClientId>| ${ identityClientId } |g\" helm-config.yaml # You can further modify the helm config to enable/disable features nano helm-config.yaml Values: - verbosityLevel : Sets the verbosity level of the AGIC logging infrastructure. See Logging Levels for possible values. - appgw.subscriptionId : The Azure Subscription ID in which App Gateway resides. Example: a123b234-a3b4-557d-b2df-a0bc12de1234 - appgw.resourceGroup : Name of the Azure Resource Group in which App Gateway was created. Example: app-gw-resource-group - appgw.name : Name of the Application Gateway. Example: applicationgatewayd0f0 - appgw.shared : This boolean flag should be defaulted to false . Set to true should you need a Shared App Gateway . - kubernetes.watchNamespace : Specify the name space, which AGIC should watch. This could be a single string value, or a comma-separated list of namespaces. - armAuth.type : could be aadPodIdentity or servicePrincipal - armAuth.identityResourceID : Resource ID of the Azure Managed Identity - armAuth.identityClientId : The Client ID of the Identity. See below for more information on Identity - armAuth.secretJSON : Only needed when Service Principal Secret type is chosen (when armAuth.type has been set to servicePrincipal ) - rbac.enabled : Make sure to set this to true if you have a AKS cluster that is RBAC enabled. Note on Identity: The identityResourceID and identityClientID are values that were created during the Create an Identity steps, and could be obtained again using the following command: az identity show -g <resource-group> -n <identity-name> - <resource-group> in the command above is the resource group of your App Gateway. - <identity-name> is the name of the created identity. All identities for a given subscription can be listed using: az identity list Install the Application Gateway ingress controller package: helm install -f helm-config.yaml application-gateway-kubernetes-ingress/ingress-azure","title":"Install Ingress Controller Helm Chart"},{"location":"setup/install-new/#install-a-sample-app","text":"Now that we have App Gateway, AKS, and AGIC installed we can install a sample app via Azure Cloud Shell : cat <<EOF | kubectl apply -f - apiVersion : v1 kind : Pod metadata : name : aspnetapp labels : app : aspnetapp spec : containers : - image : \"mcr.microsoft.com/dotnet/core/samples:aspnetapp\" name : aspnetapp-image ports : - containerPort : 80 protocol : TCP --- apiVersion : v1 kind : Service metadata : name : aspnetapp spec : selector : app : aspnetapp ports : - protocol : TCP port : 80 targetPort : 80 --- apiVersion : extensions/v1beta1 kind : Ingress metadata : name : aspnetapp annotations : kubernetes.io/ingress.class : azure/application-gateway spec : rules : - http : paths : - path : / backend : serviceName : aspnetapp servicePort : 80 EOF Alternatively you can: Download the YAML file above: curl https://raw.githubusercontent.com/Azure/application-gateway-kubernetes-ingress/master/docs/examples/aspnetapp.yaml -o aspnetapp.yaml Apply the YAML file: kubectl apply -f aspnetapp.yaml","title":"Install a Sample App"},{"location":"setup/install-new/#other-examples","text":"The tutorials document contains more examples on how toexpose an AKS service via HTTP or HTTPS, to the Internet with App Gateway.","title":"Other Examples"}]}